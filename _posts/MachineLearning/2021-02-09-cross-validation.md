---
layout: post
title: "[Machine Learning] Cross Validation"
date: 2021-02-09
category: [Machine Learning]
MachineLearning : true 
excerpt: ""
tags: []
comments: true
---





Cross Validation

여러 다른 머신러닝 모델을 비교하고 성능을 평가할 수 있게 해준다



우리가 수집한 타겟 데이터 컬럼

1) train the machine learning algorithm : 머신러닝 알고리즘을 학습시킨다.  estimate the parameters모델의 파라미터를 측청한다

2) test the machine learning algorithm :머신러닝 알고리즘을 테스트한다. 머신러닝 모델이 얼마나 잘 작동하는지 평가



학습 데이터와 테스트 데이터를 4 블럭으로 나눠서 3 블럭은 학습 데이터로 쓰고, 1 블럭은 테스트를 하는데 쓴다.(4-Fold-Cross Validation)  테스트에 쓸 블럭을 바꿔가며 얼마나 잘 예측했는지 측정하여 다른 모델과 비교한다. Leave-one-out-Cross Validation은 모든 각각의 샘플을 블럭이라고 가정하고, 한 샘플을 테스트에 쓴다. 보통은 10 블럭으로 나누는 10-Fold Cross validation을 많이 사용한다. 



하이퍼 파라미터가 있는 모델의 경우 10-Fold-Cross Validation을 파라미터를 튜닝하는 데 사용한다.

파라미터 하나가 바뀌면 다른 모델로 취급

오늘 배운 거는 파라미터 튜닝하는 것인데 나중에 score로 아예 다른 모델(다른 알고리즘 쓰는 모델, 랜덤포레스트, 로지스틱...등)도 비교 가능하다.



랜덤서치cv를 사용할 때 refit을 써야하는지?
refit을 true로 하면 바로 검증+훈련 세트로 훈련시킴

랜덤서치를 써보고 좀 근처 값으로 그리드 서치를 해도 되는데 꼭 올라가는 건 아니다.
verbose 1 높이면 cv하는 상황이 조금 더 구체적으로 나온다. 코드 진행 상황이 보임

특징이 중요하면 데이터 클리닝의 영향을 많이 받음. 안 중요하면 별 영향 없다
일단 간단하게 라도 처리하고 모델 돌려보고, feature engineering 하기
컴퓨터를 괴로히는 것이 더 나을 수 있다..

랜덤 스테이트의 하이퍼 파라미터ㅋㅋ
대회니까 테스트 데이터에 맞출려고 넣어서 돌릴 수는 있음. 하지만 현실적이진 않음

타겟의 메이저 클래스가 많으면 f1 score 영향 많이 주게 됨. 이 때 클래스 웨이트를 좀 바꿔 볼 수도 있음

회귀 문제에서 주로 타겟 인코딩 사용
타겟 인코딩은 훈련 데이터에만 씀. 이걸 스무딩해서 테스트 셋에 인코딩을 할 때는 훈련데이터에 fit한 걸로 transform함

k fold, holdout 어떤 게 좋음?
일단 cv 쓰고, 데이터가 정말 많으면 holdout 써도 됨
데이터가 너무 많으면 cv하는데 시간 엄청 걸림
데이터가 적으면 cv
기준은 본인 컴퓨터 사양에 따라;;;

recall 줄고 precision 늘리는 방향으로 수정 가능?
가능. 특성 전처리 등 가능. 테스트 셋은 마지막에 한번 측정.

클래스 웨이트 : 모델 자체에 타겟 클래스 비중이 안 맞을 때. 비율이 적은 클래스에 모델에서 가중치
f beta score : beta가 크면 recall의 영향을 많이 받게 스코어가 나옴. 리콜이 안 좋으면 스코어가 낮게 나옴. 모델은 다 만들어진 상태에서 스코어만 좀 다르게 보는 것. 

보통 테스트 데이터 스코어가 검증 데이터 스코어보다 엄청 낮으면 문제
테스트 데이터의 f1 score를 보고 계속 돌아가서 고치면 결국 검증 데이터처럼 쓰게 되면
테스트 셋에 과적합되어서 테스트 테이터의 신뢰가 무너지는 것

검증 셋에서 높은 f1 score가 나오는 걸로 내기. 테스트 셋 f1 score 높아도 private leader board에서 높지 않을 수 있음.

특성별로 encoder 다른 거 써도 된다. 
--그럼 binary는 원핫 써볼까?

실무에서 데이터 언제 업데이트?
하루에 한번 ? 실시간? 일주일에 한번 등 훈련함

특성들끼리 상관관계 분석을 했는데...?타겟변수랑 크게 유의한 변수가 없는 것 같아서
그냥 다 썼다. selectKBest도 상관관계 분석, 히트맵보다 kbest. ->이것도 randomsearch에 넣고 쓰기

히트맵 상관관계 corrcoef는 하나의 특성과 하나의 특성의 관계임. 따라서 여러 특성 간의 관계를 보여주지 못함. 

파라미터 중 효율성을 위한 것이 있음. 컴퓨터 시간 줄이려고 특성을 줄이기도 함.

같이 써도 되는 데이터인가 고민..근데 그 데이터 자체의 이해가 있어야 함.



선형모델에서는 다중공선성 조심. 트리모델에는 그런 고민이 좀더 적어짐.
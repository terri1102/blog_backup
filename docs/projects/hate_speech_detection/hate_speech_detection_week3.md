---
layout: default
title: Hate speech detection í”„ë¡œì íŠ¸ week 3
grand_parent: Projects
parent: Hate speech detection í”„ë¡œì íŠ¸
has_children : true
nav_order: 3
---



### Week3



## Sentence transformer ëª¨ë¸ ê³ ë¥´ê¸°

feature extractionì¸ ê²ƒ ì¤‘ ê°€ë²¼ìš´ ëª¨ë¸ë¡œ ê³ ë¥´ê¸°

ê·¼ë° mini ë¶™ì€ ê±´ ë‹¤ sentence similarity ì—¬ì„œ ê·¸ëƒ¥ minië¡œ ì¨ë³´ê¸°

### Reference



## ëª¨ë¸ êµ¬ì¡°

context sentenceë¥¼ ì–´ë–»ê²Œ BERT ëª¨ë¸ê³¼ ë¶™ì¼ê¹Œ ê³„ì† ì´ ê³ ë¯¼ë§Œ í•˜ê³  ìˆë‹¤...

ê·¸ëŸ¬ë‹¤ê°€ multiple inputsì´ë¼ëŠ” ì´ ê¸€ì—ì„œ íŒíŠ¸ë¥¼ ì–»ì–´ì„œ ëª¨ë¸ì„ êµ¬ìƒí•´ë³´ì•˜ë‹¤. ì´ ëª¨ë¸ì´ ì•„ì§ ëŒì•„ê°ˆì§€ ì•ˆ ê°ˆì§€ëŠ” ëª¨ë¥´ê² ë‹¤.

https://discuss.pytorch.org/t/nn-module-with-multiple-inputs/237/2



```
#print(model)
Some weights of the model checkpoint at google/electra-small-discriminator were not used when initializing ElectraModel: ['discriminator_predictions.dense_prediction.weight', 'discriminator_predictions.dense.weight', 'discriminator_predictions.dense_prediction.bias', 'discriminator_predictions.dense.bias']
- This IS expected if you are initializing ElectraModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ElectraModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
```



```
HateSpeechClassifier(
  (electra): Sequential(
    (0): ElectraModel(
      (embeddings): ElectraEmbeddings(
        (word_embeddings): Embedding(30522, 128, padding_idx=0)
        (position_embeddings): Embedding(512, 128)
        (token_type_embeddings): Embedding(2, 128)
        (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (embeddings_project): Linear(in_features=128, out_features=256, bias=True)
      (encoder): ElectraEncoder(
        (layer): ModuleList(
          (0): ElectraLayer(
            (attention): ElectraAttention(
              (self): ElectraSelfAttention(
                (query): Linear(in_features=256, out_features=256, bias=True)
                (key): Linear(in_features=256, out_features=256, bias=True)
                (value): Linear(in_features=256, out_features=256, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): ElectraSelfOutput(
                (dense): Linear(in_features=256, out_features=256, bias=True)
                (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): ElectraIntermediate(
              (dense): Linear(in_features=256, out_features=1024, bias=True)
            )
            (output): ElectraOutput(
              (dense): Linear(in_features=1024, out_features=256, bias=True)
              (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (1): ElectraLayer(
            (attention): ElectraAttention(
              (self): ElectraSelfAttention(
                (query): Linear(in_features=256, out_features=256, bias=True)
                (key): Linear(in_features=256, out_features=256, bias=True)
                (value): Linear(in_features=256, out_features=256, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): ElectraSelfOutput(
                (dense): Linear(in_features=256, out_features=256, bias=True)
                (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): ElectraIntermediate(
              (dense): Linear(in_features=256, out_features=1024, bias=True)
            )
            (output): ElectraOutput(
              (dense): Linear(in_features=1024, out_features=256, bias=True)
              (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (2): ElectraLayer(
            (attention): ElectraAttention(
              (self): ElectraSelfAttention(
                (query): Linear(in_features=256, out_features=256, bias=True)
                (key): Linear(in_features=256, out_features=256, bias=True)
                (value): Linear(in_features=256, out_features=256, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): ElectraSelfOutput(
                (dense): Linear(in_features=256, out_features=256, bias=True)
                (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): ElectraIntermediate(
              (dense): Linear(in_features=256, out_features=1024, bias=True)
            )
            (output): ElectraOutput(
              (dense): Linear(in_features=1024, out_features=256, bias=True)
              (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (3): ElectraLayer(
            (attention): ElectraAttention(
              (self): ElectraSelfAttention(
                (query): Linear(in_features=256, out_features=256, bias=True)
                (key): Linear(in_features=256, out_features=256, bias=True)
                (value): Linear(in_features=256, out_features=256, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): ElectraSelfOutput(
                (dense): Linear(in_features=256, out_features=256, bias=True)
                (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): ElectraIntermediate(
              (dense): Linear(in_features=256, out_features=1024, bias=True)
            )
            (output): ElectraOutput(
              (dense): Linear(in_features=1024, out_features=256, bias=True)
              (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (4): ElectraLayer(
            (attention): ElectraAttention(
              (self): ElectraSelfAttention(
                (query): Linear(in_features=256, out_features=256, bias=True)
                (key): Linear(in_features=256, out_features=256, bias=True)
                (value): Linear(in_features=256, out_features=256, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): ElectraSelfOutput(
                (dense): Linear(in_features=256, out_features=256, bias=True)
                (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): ElectraIntermediate(
              (dense): Linear(in_features=256, out_features=1024, bias=True)
            )
            (output): ElectraOutput(
              (dense): Linear(in_features=1024, out_features=256, bias=True)
              (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (5): ElectraLayer(
            (attention): ElectraAttention(
              (self): ElectraSelfAttention(
                (query): Linear(in_features=256, out_features=256, bias=True)
                (key): Linear(in_features=256, out_features=256, bias=True)
                (value): Linear(in_features=256, out_features=256, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): ElectraSelfOutput(
                (dense): Linear(in_features=256, out_features=256, bias=True)
                (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): ElectraIntermediate(
              (dense): Linear(in_features=256, out_features=1024, bias=True)
            )
            (output): ElectraOutput(
              (dense): Linear(in_features=1024, out_features=256, bias=True)
              (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (6): ElectraLayer(
            (attention): ElectraAttention(
              (self): ElectraSelfAttention(
                (query): Linear(in_features=256, out_features=256, bias=True)
                (key): Linear(in_features=256, out_features=256, bias=True)
                (value): Linear(in_features=256, out_features=256, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): ElectraSelfOutput(
                (dense): Linear(in_features=256, out_features=256, bias=True)
                (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): ElectraIntermediate(
              (dense): Linear(in_features=256, out_features=1024, bias=True)
            )
            (output): ElectraOutput(
              (dense): Linear(in_features=1024, out_features=256, bias=True)
              (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (7): ElectraLayer(
            (attention): ElectraAttention(
              (self): ElectraSelfAttention(
                (query): Linear(in_features=256, out_features=256, bias=True)
                (key): Linear(in_features=256, out_features=256, bias=True)
                (value): Linear(in_features=256, out_features=256, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): ElectraSelfOutput(
                (dense): Linear(in_features=256, out_features=256, bias=True)
                (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): ElectraIntermediate(
              (dense): Linear(in_features=256, out_features=1024, bias=True)
            )
            (output): ElectraOutput(
              (dense): Linear(in_features=1024, out_features=256, bias=True)
              (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (8): ElectraLayer(
            (attention): ElectraAttention(
              (self): ElectraSelfAttention(
                (query): Linear(in_features=256, out_features=256, bias=True)
                (key): Linear(in_features=256, out_features=256, bias=True)
                (value): Linear(in_features=256, out_features=256, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): ElectraSelfOutput(
                (dense): Linear(in_features=256, out_features=256, bias=True)
                (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): ElectraIntermediate(
              (dense): Linear(in_features=256, out_features=1024, bias=True)
            )
            (output): ElectraOutput(
              (dense): Linear(in_features=1024, out_features=256, bias=True)
              (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (9): ElectraLayer(
            (attention): ElectraAttention(
              (self): ElectraSelfAttention(
                (query): Linear(in_features=256, out_features=256, bias=True)
                (key): Linear(in_features=256, out_features=256, bias=True)
                (value): Linear(in_features=256, out_features=256, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): ElectraSelfOutput(
                (dense): Linear(in_features=256, out_features=256, bias=True)
                (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): ElectraIntermediate(
              (dense): Linear(in_features=256, out_features=1024, bias=True)
            )
            (output): ElectraOutput(
              (dense): Linear(in_features=1024, out_features=256, bias=True)
              (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (10): ElectraLayer(
            (attention): ElectraAttention(
              (self): ElectraSelfAttention(
                (query): Linear(in_features=256, out_features=256, bias=True)
                (key): Linear(in_features=256, out_features=256, bias=True)
                (value): Linear(in_features=256, out_features=256, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): ElectraSelfOutput(
                (dense): Linear(in_features=256, out_features=256, bias=True)
                (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): ElectraIntermediate(
              (dense): Linear(in_features=256, out_features=1024, bias=True)
            )
            (output): ElectraOutput(
              (dense): Linear(in_features=1024, out_features=256, bias=True)
              (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (11): ElectraLayer(
            (attention): ElectraAttention(
              (self): ElectraSelfAttention(
                (query): Linear(in_features=256, out_features=256, bias=True)
                (key): Linear(in_features=256, out_features=256, bias=True)
                (value): Linear(in_features=256, out_features=256, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): ElectraSelfOutput(
                (dense): Linear(in_features=256, out_features=256, bias=True)
                (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): ElectraIntermediate(
              (dense): Linear(in_features=256, out_features=1024, bias=True)
            )
            (output): ElectraOutput(
              (dense): Linear(in_features=1024, out_features=256, bias=True)
              (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
        )
      )
    )
    (1): Dropout(p=0.3, inplace=False)
  )
  (lstm): Sequential(
    (0): Embedding(66000, 3)
    (1): LSTM(128, 2)
  )
  (out): Linear(in_features=256, out_features=2, bias=True)
)
```



ë¬¸ì œ concatí•  ë•Œ ë²¡í„°ì‚¬ì´ì¦ˆ ì•ˆ ë§ìŒ

https://discuss.pytorch.org/t/concatenation-of-the-hidden-states-produced-by-a-bidirectional-lstm/3686

-> L2 poolingì„ í•´ë¼





ì„±ëŠ¥ ë„ˆë¬´ ì•ˆ ë‚˜ì˜¤ë©´ í´ë˜ìŠ¤ ë¹„ìœ¨ ì¡°ì •. ë°ì´í„°ì…‹ êµì²´



í´ë˜ìŠ¤ ë¹„ìœ¨ì´ ì²˜ì°¸í•˜ë‹¤.  70:1 ì •ë„

ê·¸ë˜ì„œ NLPì˜ oversamplingì— ëŒ€í•´ ì°¾ì•„ë³´ì•˜ë‹¤. í…ìŠ¤íŠ¸ representationì„ êµ¬í•œë‹¤ìŒì— resamplingí•˜ëŠ” ê²ƒ ê°™ë‹¤.

https://simonezz.tistory.com/92?category=892980



Data augmentation

https://towardsdatascience.com/how-i-handled-imbalanced-text-data-ba9b757ab1d8

1. documentë¥¼ tokenizeí•´ì„œ sentenceë¡œ ë§Œë“¦
2. ì´ë¥¼ shuffleí•˜ê³  rejoiní•´ì„œ ìƒˆë¡œìš´ textë¥¼ ë§Œë“¦
3. ì•„ë‹ˆë©´ í˜•ìš©ì‚¬, ë™ì‚¬ ë“±ì„ ìœ ì˜ì–´ë¡œ ë°”ê¿ˆ
4. ìœ ì˜ì–´ë¡œ ë°”ê¿€ ë•Œ pre-trained word embeddingì´ë‚˜ NLTKì˜ wordnet ë“± ì‚¬ìš©



ì•„ë‹ˆë©´ ë²ˆì—­ í›„ì— ë‹¤ì‹œ ì˜ì–´ë¡œ ë²ˆì—­í•¨

ì—¬ê¸° ê¹ƒí—™ ë³´ê³  ì„¸ ê°€ì§€ ë°©ë²• ì°¸ê³ í•´ë³´ê¸°

https://github.com/kothiyayogesh/medium-article-code/blob/master/How%20I%20dealt%20with%20Imbalanced%20text%20dataset/data_augmentation_using_language_translation.ipynb



ë¯¸ë””ì—„ ê¸€ ë§‰íˆë©´ ì‹œí¬ë¦¿ ì°½ìœ¼ë¡œ ë³´ê¸°



ë¬¸ì œ: ë‚´ê°€ ì°¸ê³ í•œ ALBERTëª¨ë¸ì„ ì´ìš©í•œ sentence pair classification ì½”ë“œê°€ ELECTRAì— ì ìš©í•˜ë‹ˆê¹Œ ì˜¤ë¥˜ë‚¨



ì—ëŸ¬1: 

not enough values to unpack (expected 2, got 1)

```python
cont_reps, pooler_output = self.bert_layer(input_ids, attn_masks, token_type_ids)
```

bert layerì˜ outputì˜ ê°œìˆ˜ê°€ ELECTRAë‘ ALBERTê°€ ë‹¤ë¦„



ìœ„ì˜ ì½”ë“œë¥¼ ìˆ˜ì •

```python
pooler_output = self.bert_layer(input_ids, attn_masks, token_type_ids)

```



ì—ëŸ¬2:

```python
dropout(): argument 'input' (position 1) must be Tensor, not BaseModelOutputWithPastAndCrossAttentions
```

```python
logits = self.cls_layer(self.dropout(pooler_output))
```

ì € dropoutì— ë“¤ì–´ê°ˆ pooler_outputì´ BaseModelOutputWithPastAndCrossAttentionsì—¬ì„œ ìƒê¸´ ë¬¸ì œë‹¤. Tensorì—¬ì•¼ í•˜ëŠ”ë°..



ì¦‰ ì—ëŸ¬1ê³¼ ì—ëŸ¬2ëŠ” ì—°ê´€ ë˜ì–´ ìˆëŠ”ë°, ELECTRA ëª¨ë¸ì˜ outputì´ BaseModelOutputWithPastAndCrossAttentions ê°ì²´ í•˜ë‚˜ë§Œ ë‚˜ì˜¤ê¸° ë–„ë¬¸ì— ë‘ ì—ëŸ¬ê°€ ë°œìƒí•œ ê²ƒì´ë‹¤. 



í•´ê²°: x

configì—ì„œ return_dict=Falseë¡œ ì„¤ì •í•˜ë©´ BaseModelOutputWithPastAndCrossAttentionsê°€ ì•„ë‹Œ tensorì˜ tupleì´ ë‚˜ì˜¨ë‹¤.

from transformers import ElectraConfig, PretrainedConfig ë” ì•Œì•„ë³´ê¸°



https://huggingface.co/transformers/v2.9.1/main_classes/configuration.html#transformers.PretrainedConfig



configureí•˜ê¸°

```python
config = ElectraConfig.from_pretrained('google/electra-small-discriminator') 
config.return_dict = False
model = ElectraModel(config)
```



---



7ì›” 3ì¼ì˜ ê²°ê³¼

## 1. ë°ì´í„°ì…‹ ì„ ì •

Toxicity Detection: Does Context Really Matter?

10000ê°œì˜ ìƒ˜í”Œ. ì—„ì²­ë‚œ í´ë˜ìŠ¤ ë¶ˆê· í˜•

0    0.9849 1    0.0151

A Benchmark Dataset for Learning to Intervene in Online Hate Speech: Gab, Reddit

Gabì—ì„œ 3266ê°œì˜ í˜ì˜¤ ë°œì–¸ ì¼ë‹¨ ì‚¬ìš© (parent: non hate, text: hate)

GPT-NEO

## 2. ë°ì´í„° ì „ì²˜ë¦¬

https://~~ â†’ url

@dfdfjkejlp â†’ username

ğŸ€ â†’ 'rat'

## 3. ëª¨ë¸

Albert base v2: sentence pair classification ëª¨ë¸, sentencepiece tokenizer

## 4. ê²°ê³¼

Toxicity Detection: Does Context Really Matter?

{'accuracy': 0.985, 'f1': 0.0} â†’ í´ë˜ìŠ¤ ë¶ˆê· í˜•ì´ ì‹¬í•´ì„œ í•œìª½ìœ¼ë¡œë§Œ ì°ëŠ” ëª¨ë¸

A Benchmark Dataset for Learning to Intervene in Online Hate Speechë¥¼ ì„ì€ ë°ì´í„°ì…‹

{'accuracy': 0.9868421052631579, 'f1': 0.974396488661302} â†’ ë„ˆë¬´ ë†’ì•„ì„œ ë­”ê°€ ì˜ëª»ëœë“¯..

167ê°œ ìƒ˜í”Œ ì˜ˆì¸¡í•˜ëŠ”ë° ê±¸ë¦° ì‹œê°„: 60.3411ì´ˆ

ëŒ€ëµ 1ê°œë‹¹ 0.3613ì´ˆ

50msëŠ” ì–´ë–»ê²Œ...?



Reddit ì „ì²˜ë¦¬ë¥¼ í•  ë•Œ [deleted] ê°™ì€ ê±°ë‘ ì™¸êµ­ì–´ ì²˜ë¦¬!

---





Inference time ì¬ëŠ” ë²•:https://towardsdatascience.com/the-correct-way-to-measure-inference-time-of-deep-neural-networks-304a54e5187f


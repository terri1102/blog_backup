---
layout: default
title: Langcon2021 후기
date: 2021-09-02
parent: etc
nav_order: 11
comments: true
---





토크나이징이란?

문장을 단어로 자르는 것이 가장 간단한 방식

하지만 한국어는 어렵다.

Scriptio Continua인 언어들은 (e.g. CJK) 형태소 등의 단위로 분절하는 알고리즘을 이용, 따라서 데이터셋이 필요하고 구현 또한 복잡. 한국어는 어절이 단위임

subword 이전 한국어의 de-facto 표준은 형태소 기반 분절(konlpy, khaii) 카이 많이 쓴다고 함. MEcab을 제일 많이 씀.

전이학습에 있어 전이학습 후 최종 평가 성능을 좌우하는 첫번째 요소

다국어BERT(mBERT)가 한국어에 애매한 모델...한국어랑 토크나이징 방법이 안 맞아서



한국어의 경우 토큰은 어절, 형태소, 서브워드, 음절, 바이트 단위로 분절하여 사용 가능

* 표현능력(Representation Robustness), 토큰의 길이, OOV에 대한 안전성 3가지 측면에서 균형이 필요
* 표현 능력은 의미 경계가(형태소 등) 우수한 성향을 보이나 형태소 기반 토크나이징이 완벽하지 않아 애매한 표현을 학습할 위험
* 토큰이 너무 길어지면 merge가 안 되고, orphan이 됨

* 토큰의 길이와 OOV에 대한 안정성은 반비례함



**단위별 알고리즘**

음절: 한국어에선 한 음절, 영어는 캐릭터 레벨

FASttext: 서브워드 기반이지만 해싱 사용하는 것이 특이함



**사전 분절(pre-tokenization)**

BERT 같은 경우은 어절 단위 분절 후 워드 피스 알고리즘 적용. 정확하게 공개되어 있지는 않음. 중국어, 일본어의 경우 한자는 문자 단위로 토크나이징하는 편법 사용(논문에는 기재 안 됨)

* 일본어와 같이 형태소 기반 사전 분절 기법이 한국어에서도 효과적인 것으로 확인(Park et al., 2020)
* 전처리로 자모 단위 분해 후 BPE를 학습하는 기법 또한 제안된 바 있음



**한국어의 NLP가 어려운 이유**

맞춤법 일부러 틀리는 경우, 띄어쓰기 틀리는 경우

의미있는 단위로 자르려면 형태소 분석기가 필요함 -> 신조어와 구어식 표현에 취약

'세종'이라는 코퍼스 사용



**국가표준 MeCab과 21세기 세종 계획**

Mecab-ko가 현재 가장 일반적 

mecab-ko: 2018년 7월 최신 모델 출시가 마지막

단, 세종 코퍼스는 수정/재배포가 엄격히 금지되어 있음.

**모두의 말뭉치 또한 유사한 라이센스 제약이 있음!!**

라이센스 문제로 형태소 기반 토크나이징의 발전이 정체된 상태



---

자모 기반 서브워드

문제의식: 왜 한글은 알파벳이 아닌 음절을 기반으로 서브워드를 학습하는가?

KSC5601-1987로 2음절 서브워드 구성시 총 11,045,000 서브워드

방법 : 서브워드 학습 전에 자모 단위로 분해 후 학습, 생성의 경우 자모 단위로 출력된 것을 다시 후처리(유니그램 lm, sentencepiece로 학습)

문제: 출력이 자모 단위로 나올 경우 (생성) 대응이 안되는 제약이 있음

입력기 오토마타를 응용하여 생성 문제에 대한 해결책 포함하여 제안. 하지만 출력하나 잘못 나오면 ...문제

Q.생성모델을 자모단위로 할 때의 장점?

A. speech synthesize를 할 때는 장점이 있음. 근데 생성 자체를 자모로 할 때의 장점은..잘 모르겠음. 입력에 있어서 OOV를 최소화시키는 목적이 더 강했음.

Q. 초성,중성,종성을 다른 자소로 분리하여 해본 적이 있었는데, 그렇게 하면 오류가 좀 줄지 않을까?

A. 네. 오류가 좀 줄어듭니다. 종성 자모 유니코드가 다른 게 이미 있음





모르는 단어 나중에 배우기, PatchBERT

최종 태스크 파인튜닝 직전에 "모르는 단어 학습" 단계 추가

모르는 단어 파악 후, 기존 단어장에 존재하는 다른 서브워드의 임베딩과 weight sharing을 하거나 덮어쓰는 식(pre-training을 좀 더 하는 식)

심각하게 망가진 모델도 어느 정도 복구 가능한 것 확인(일부러 단어장 줄이고..)



Q. 영어 모델에 한국어 코퍼스 학습하면 둘 다 가능한 모델?

A. task는 풀수 있겠지만 바이 링구얼 모델이라고 보기는 어렵지 않을까?

---



# 한국어 토크나이징의 미래

오픈 형태소 분석 데이터셋, OpenKorpos

기 학습 형태소 분석기의 Consensus를 이용, 기계 태깅하여 제작

ACL에서 고배마신 이유: human evaluation이 부족함

5천 5백만 어절 규모. 전량 수동 평가는 불가

저작권에 대해 자유로운 문장을 투고할 수 있고, 문제점들을 Github pR로 수정할 수 있도록 오픈소스 프로젝트로 Living corpus를 지향

특정 스냅샷 기준으로 성능 평가가 비교 가능하도록 할 예정

조만간 공개 예정



형태소 분석 태스크를 분리해서 본다면?

분절과 POS 태깅을 같이 해야만 하는 특성 때문에 형태소 분석용 말뭉치를 만드는 것은 공수가 많이 들어감.(JKG와 JKO의 차이?)

그러나 현실에서 형태소 분석기를 사용할 경우 분절된 형태소만 사용하고 POS 태그는 버려지는 경우가 대부분.

형태소 분절만 하는 데이터셋/ 모델을 만드는 것을 목표로 하면 조금 더 저렴하고 효율적으로 만들 수 있까에 대한 고민..(segmentation과 classification을 따로 )

Mecab은 완벽한 형태소 분석이 아님



신조어에 대한 대응

일본어의 경우 neologd라는 프로젝트가 있으며, 신조어 학습을 하기 위한 원부 데이터와 신조어 기학습 사전으로 제공하는 오픈소스 프로젝트

여기서 착안하면 좋지 않을까?



그 외 아이디어

Subword Regularization을 한국어에 적용할 때와 하지 않았을 때의 차이는?

Differentiable Tokenization을 한국어에 적용한다면?

JUMAN++같이 좋은 형태소 분석기를 한국어 데이터셋에 학습을 시도해본다면?

자모 기반 서브워드를 생성 모델에서 사용시, 다음 서브워드가 잘못 나오는 경우에 대한 대책은?

Hash embedding에 대한 접근?

질문: sangwhan@iki.fi 또는 트위터 @sangwhanmoon



---

# KLUE 

Upstage 문지형



KLUE Paper Day 에 발표를 했었음. 유튜브로 공개됨



KLUE의 제작 목적

* 자연어 이해 모델의 평가



KLUE Task: 8가지

문장 유사도, 문장 주제 분류, **대화 상태 추적**! 나중에 찾아보기



KLUE 구축 과정

* 어떤 태스크를 선정해야 할까?

  NLU 필수 태스크인가?

  다른 Benchmark와의 차별성이 있는가?

  한국어만의 특성이 고려되어야 하는 태스크인가?

  저작권 이슈가 없는가?

  ...

투표를 통해서 결정



선정된 태스크에 분배

구어체와 문어체가 섞여있거나, 구어체로 구축이 가능한 태스크인가?

문어/구어 Corpora 중에 해당 태스크에 적합한 코퍼스는 무엇인가?

위키트리: 뉴스 컨텐츠의 퀄리티 이슈 때문에 DP, NER 등

위키피디아: MRC를 제작하기 위해서는 문단 단위의 텍스트가 꼭 필요



태스크에 적합한 데이터 수량은 어느 정도인가?



WoS

dropdown menu: 에러방생 최소화



어노테이션 퀄리티 향상

가이드라인의 모호함을 최대한 제거: 여러번의 iteration

꼼꼼한 검수

제작자의 의도를 명료하게 전달



NER: 특정 단어와 label을 연관짓는 모델 x -> 문맥 속에서 단어의 entity를 판단할 수 있게. 즉 test, train set의 오버랩을 줄이려고 했음



매트릭 선정

MRC 

ROUGE-W: 토크나이제이션 성능의 영향을 적게 받는 metric

​					우연히 chracter가 겹치는 단어를 정답으로 잡은 경우데 대해 과도하게 점수를 주지 않는 매트릭



DP macro f1 class imbalnce가 심하기 때문



**Baseline 코드 소개(레포지토리에 공개됨)**



**KLUE 리더보드**



---

KLUE 이후의 데이터(에 대한 개인적인 바람)



모델의 발전을 견인하는 데이터: 이 데이터가 있음으로 인해 못풀던 영역의 문제를 풀 수 있게 된 경우

BlenderBot 2.0: 별도의 태스크와 블랜딩 태스크가 있음. 

데이터에 대한 모델 점수의 향상이 곧 모델의 성능 향상으로 이어지는 데이터

Baseline 모델의 점수가 낮아서 점수 향상의 여유가 있는 데이터

static benchmark vs. dynamic benchmark(페이스북 논문으로 있음)



Q. 모델 학습까지 필요한 데이터 양은?

A. 이미 공개된 데이터셋과 모델의 성능을 보고 유추함. 번역 같은 경우는 거리가 먼 데이터셋 많이 모으려고 함. 양과 질의 트레이드 오프가 있긴 함. 수량 같은 경우는 조금씩 추가하면서 성능이 얼마나 오르는지 확인(리니어 하게 오르면 더 넣어도 되겠군. 포화상태면 더 필요 없겠군)



---

# 말로하는 감정 인식

바벨피쉬(NLP 스터디) 송치성



Speech emotion recognition(SER)



감정(Emotion)? 

Mood(우울하다..),  Sentiment(어떤 대상에 대한 약한/강한 긍정/부정 반응)과 다름



연속적 표현, 이산적 표현으로 가능

* Continuous: Arousal(감정의 강도가 큰지/작은지), Valence: 긍정인지 부정인지
* Discrete: 감정을 이산적 카테고리로 표현(angry, sad, happy)



음성인식 아키텍처

![image-20210828143555977](C:\Users\Boyoon Jang\AppData\Roaming\Typora\typora-user-images\image-20210828143555977.png)


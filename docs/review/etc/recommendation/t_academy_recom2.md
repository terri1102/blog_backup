---
layout: default
title: 추천시스템의 이해2
date: 2021-08-06
grand_parent: Reviews
parent: ETC
comments: true
nav_order: 12
---





# 4강 협업 필터링

**협업 필터링** : 사용자의 구매 패턴이나 평점을 가지고 다른 사람들의 구매 패턴, 평점을 통해 추천. 나와 비슷한 사람을 찾아서 그들의 선호를 이용한 추천.



장점: 사용자의 개인정보나 아이템의 정보가 없어도 추천 가능



**종류**

* 최근접 이웃기반(KNN)

* 잠재 요인기반(ALS)



## Neighborhood based method

* 정의: Neighborhood based Collaborative Filtering은 메모리 기반 알고리즘으로 협업 필터링을 위해 개발된 초기 알고리즘

* 알고리즘

  * User-based collaborative filtering : 사용자의 구매패턴(평점)과 `유사한 사용자`를 찾아서 추천 리스트 생성
  * Item-based collaborative filtering: 특정 사용자가 준 점수간의 `유사한 상품`을 찾아서 추천 리스트 생성

  

## KNN(K Nearest Neighbors)

가장 근접한 K명의 Neighbors를 통해서 예측하는 방법



**예시**

<br>

데이터(Explicit Feedback) : 유저가 자신의 선호도를 직접 표현한 데이터 <br>

<-> implicit feedback: 구매 정보만 알고 만족도나 선호도는 없는 데이터



### User Based Collaborative Filtering <br>

유저가 자신의 선호도를 직접적으로 표현한 데이터 <br>

사용자 3을 기준으로 유사도 구해봄. ???는 무시하고 계산함

|         | 아이템1 | 아이템2 | 아이템3 | 아이템4 | 아이템5 | 아이템6 | 평균 | Cosine(i,3) | Pearson(i,3) |
| ------- | ------- | ------- | ------- | ------- | ------- | ------- | ---- | ----------- | ------------ |
| 사용자1 | 7       | 6       | 7       | 4       | 5       | 4       | 5.5  | 0.956       | 0.894        |
| 사용자2 | 6       | 7       | ???     | 4       | 3       | 4       | 4.8  | 0.981       | 0.939        |
| 사용자3 | ???     | 3       | 3       | 1       | 1       | ???     | 2    | 1.0         | 1.0          |
| 사용자4 | 1       | 2       | 2       | 3       | 3       | 4       | 2.5  | 0.789       | -1.0         |
| 사용자5 | 1       | ???     | 1       | 2       | 3       | 3       | 2    | 0.645       | -0.817       |



문제점: 사용자 3-5는 평점이 짜고, 사용자 1,2는 평점이 후한 편 ->

<span style="background-color: #ffe5b8">bias term을 제거하는 것이 필요함</span>



* bias 제거 전

$$
\hat{\gamma_{31}} = \frac{7*0.894 + 6*0.939}{0.894+0.939} \approx 6.49 \\
\hat{\gamma_{36}} = \frac{4*0.894 + 4*0.939}{0.894+0.939} \approx 4
$$

* bias 제거 후


$$
\hat{\gamma_{31}} = 2+\frac{1.5*0.894 + 1.2*0.939}{0.894+0.939} \approx 3.35 \\
\hat{\gamma_{36}} = 2+ \frac{-1.5*0.894 + -0.8*0.939}{0.894+0.939} \approx 0.86
$$

* 1.5: 사용자1의 아이템1의 평점 - 사용자1의 평균 평점 -> 사용자1의 bias term이 제거됨
* 1.2: 사용자2의 어이템1 평점 - 사용자2의 평균 평점 -> 사용자2의 bias term 제거
* 2 : 사용자3의 평균 평점



### Item Based Collaborative Filtering

유저가 자신의 선호도를 직접적으로 표현한 데이터

<br>

Cosine(1,아이템3)
$$
Adjusted \; Cosine(1,3) = \frac{1.5*1.5+(-1.5)*(-0.5)+(-1)*(-1)}{\sqrt{1.5^2+(-1.5)^2+(-1)^2} * \sqrt{1.5^2 + (-0.5)^2+(-1)^2}} = 0.912
$$


| 0           | 아이템1 | 아이템2 | 아이템3 | 아이템4 | 아이템5 | 아이템6 | 평균 |
| ----------- | ------- | ------- | ------- | ------- | ------- | ------- | ---- |
| 사용자1     | 1.5     | 0.5     | 1.5     | -1.5    | -0.5    | -1.5    | 5.5  |
| 사용자2     | 1.2     | 2.2     | ???     | -0.8    | -1.8    | -0.8    | 4.8  |
| 사용자3     | ???     | 1       | 1       | -1      | -1      | ???     | 2    |
| 사용자4     | -1.5    | -0.5    | -0.5    | 0.5     | 0.5     | 1.5     | 2.5  |
| 사용자5     | -1      | ???     | -1      | 0       | 1       | 1       | 2    |
| Cosine(1,j) | 1       | 0.735   | 0.912   | -0.848  | -0.813  | -0.990  | 5.5  |
| Cosine(6,j) | -0.990  | -0.622  | -0.912  | 0.829   | 0.73    | 1       | 5.5  |



여기서 bias term 제거함



**Neighborhood based method의 장단점**

* 장점
  * 간단하고 직관적인 접근 방식 때문에 구현 및 디버그가 쉬움
  * 특정 item을 추천하는 이유를 정당화하기 쉽고 item 기반 방법의 해석 가능성이 두드러짐
  * 추천 리스트에 새로운 item과 user가 추가되어도 상대적으로 안정적
* 단점
  * User 기반 방법의 시간, 속도 메모리가 많이 필요
  * 희소성 때문에 제한된 범위가 있음 
    * John의 Top-K에만 관심이 있음
    * John과 비슷한 이웃 중에서 아무도 해리포터를 평가하지 않으면, John의 해리포터에 대한 등급예측을 제공할 수가 없음

컨텐츠 기반 모델과 같이 사용해서 단점을 상호보완할 수 있음





## Latent Factor Collaborative Filtering(Matrix Factorization)

**Neighborhood 모델과의 차이**<br>

Neighborhood model: 아이템의 벡터와 유저 스페이스의 벡터의 유사도를 계산해서 비슷한 아이템이나 비슷한 유저 추천<br>

Latent Model: 유저 스페이스의 잠재 스페이스와 아이템 스페이스의 잠재 스페이스를 구한다음 내적해서 구함



* 정의: 잠재 요인 협업 필터링은 Rating Matrix에서 빈 공간을 채우기 위해서 사용자와 상품을 잘 표현하는 차원(latent factor)을 찾는 방법. 잘 알려진 행렬 분해는 추천 시스템에서 사용되는 협업 필터링 알고리즘의 한 종류. 행렬 분해 알고리즘은 사용자-아이템 상호 작용, 행렬을 두 개의 저 차원 직사각형 행렬의 곱으로 분해하여 작동. 



* 원리: 사용자의 잠재요인과 아이템의 잠재요인을 내적해서 평점 매트릭스 계산

예시) SVD, Observed Only MF, Weighted MF, SGD, ALS



### SGD

정의: 고윳값 분해(eigen value decomposition)와 같은 행렬을 대각화 하는 방법
$$
Minimize \; J = \frac{1}{2} \norm{R-UV^T}^2 \\
subject \; to: No \; constraints \; on \; U \; and \; V
$$


* U와 V의 weight가 계속 없데이트됨



Gradient Descent에 의해 편미분된 값



* Regularization: weight의 폭발/소실 방지



**예시**

<br>

Explicit Feedback된 형태의 4명 유저에 대한 3개의 아이템에 대한 평점 Matrix

<br>

**Rating Matrix**

| ???  | 3    | 2    |
| ---- | ---- | ---- |
| 5    | 1    | 2    |
|4|2|1|
|2|???|4|



1. User Latent와 Item Latent의 임의로 초기화

**User Latent(U)** 사용자 4명. 컬럼의 개수는 사용자가 알아서 정함. 보통 20개

| 0.5756 | 1.4534 |
| ------ | ------ |
|   -0.199    | -1.218       |
| 2.7292 | 0.48 |
|-0.039| -2.506|





**Item Latent (V)의 transpose** : 아이템 3개. row의 개수는 사용자가 알아서 정함. 보통 20개

| 0.3668  | -1.1078 | 1.4593 |
| ------- | ------- | ------ |
| -0.3392 | 0.8972  | 0.4528 |



**U ˙V**

| -0.2819 | 0.6663 | 1.4981 |
| ------- | ------ | ------ |
|   0.3403 |-0.8728      | -0.8421     |
| 0.8384 | -2.5933 | 4.2008 |
| 0.8354 | -2.2043 | -1.1928 |




2. Gradient Descent 진행

| ???  | 3    | 2    |
| ---- | ---- | ---- |
| 5    | 1    | 2    |
| 4| 2| 1|
|2| ? | 4|

Error : 3 - 0.6663 = 2.3337 <br>

dUser = -2.3337*[-1.1078, 0.8972] + 0.01*[0.5756, 1.4534] <br>

dItem = -2.3337*[0.5756, 1.4534] + 0.01*[-1.1078, 0.8972] <br>



Updated User Latent: [0.5756, 1.4534] - Learning rate * dUser <br>

[0.446, 1.5574] = [0.5756, 1.4534] - 0.05*[2.591, -2.0793]



Updated Item Latent = [-1.1078m 0.8972] - Learning rate * dItem <br>

[-1.0401, 1.0663] = [-1.1078, 0.8972] - 0.05* [-1.3544, -3.3828]

<br>

3. 모든 평점에 대해서 반복(epoch:1) : ??를 제외한 모든 평점에 대해서 진행. Latent space의 값을 계속 업데이트 해나감

4. 2-3의 과정을 10번 반복

**이전의 Rating Matrix**

| ???  | 3    | 2    |
| ---- | ---- | ---- |
| 5    | 1    | 2    |
| 4    | 2    | 1    |
| 2    | ???  | 4    |



**업데이트 된 Rating Matrix**

**Rating Matrix**

| 2.7458 | 2.4147  | 0.3873  |
| ------ | ------- | ------- |
| 4.2476 | 0.5806  | 2.86256 |
| 2.9181 | 2.3825  | 1.3030  |
| 2.4323 | -1.9994 | 3.26    |

???자리 채워짐 <br>

평점이 높은 1번 상품은 1번 유저에게 추천 <br>

평점이 낮은 2번 상품은 4번 유저에게 추천x <br>



**SGD의 장단점**

* 장점
  * 매우 유연한 모델로 다른 Loss function을 사용할 수 있음
  * parallelized가 가능
* 단점
  * 수렴까지 속도가 느림



### ALS 

**정의 :** 기존의 SGD가 두 개의 행렬(User Latent, Item Latent)을 동시에 최적화하는 방법이라면, ALS는 두 행렬 중 하나를 고정시키고 다른 하나의 행렬을 순차적으로 반복하면서 최적화하는 방법. 이렇게 하면 기존의 최적화 문제가 convex 형태로 바뀌기에 수렴된 행렬을 찾을 수 있음.



**알고리즘**

1. 초기 아이템, 사용자 행렬을 초기화(랜덤하게)
2. 아이템 행렬을 고정하고 사용자 행렬을 최적화 : 선형 회귀처럼 계산 
3. 사용자 행렬을 고정하고 아이템 행렬을 최적화
4. 위의 2,3 과정을 반복



**수렴** <br>
$$
\norm{u-X\beta}_2 \\
\beta = (X^TX)^{-1}X^Ty
$$
| 0| 3| 2|
|---|---|---|
| 5| 1|2|
|4|2|1|
| 2    | 0    | 4    |



???는 모두 0으로 바꿔줌


$$
\forall u_i: J(u_i) = \norm{R_i - u_i * V^T} _2 + \lambda * \norm{u_i}_2
$$

* upsidedown_a: for all, first-order logic





1. 초기 아이템, 사용자 행렬을 초기화(랜덤하게)
2. 아이템 행렬을 고정하고 사용자 행렬을 최적화 : 모든 row에 대해서 계산

$$
U_1 = (V^T * V _ \lambda I)^{-1} * V^T * R_1 = [0.3151, 3.2962]
$$



3. 사용자 행렬을 고정하고 아이템 행렬을 최적화

4. 위의 2,3 과정을 반복



**implicit package로 사용**



**ALS의 장단점**

* 장점

  * SGD보다 수렴속도가 빠름
  * parallelized 가능

* 단점

  오직 Loss squares만 사용가능





**협업필터링의 장단점**

* 장점
  * 도메인 지식이 필요하지 않음
  * 사용자의 새로운 흥미를 발견하기 좋음
  * 시작단계의 모델로 선택하기 좋음(추가적인 문맥정보가 필요없음)
* 단점
  * 새로운 아이템에 대해서 다루기가 힘듦
  * side features(고객의 개인정보, 아이템의 추가정보)를 포함시키기 어려움



Word2Vec - Skip-gram



문장 자체를 인풋으로 넣으면 뱉어낸 임베딩이 훈련된 벡터임




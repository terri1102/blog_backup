---
layout: default
title: "[NLP] HateBERT: Retraining BERT for Abusive Language Detection in English 리뷰"
date: 2021-06-24
parent: Article Reviews
grand_parent: Reviews
nav_order: 6
comments: true
---



HateBERT: Retraining BERT for Abusive Language Detection in English

Tommaso Caselli, Valerio Basile, Jelena Mitrovic, Michael Granitzer University of Groningen, University of Turin, University of Passau

https://osf.io/szv2m/

# 논문 키워드

`hate speech detection`, `RAL-E`, 

<br>



# 논문 주제 

Abusive language detection에 최적인 Bert_base모델을 re-trained한 HateBert 모델을 소개

<br>

# 데이터셋

banned communities로 부터 수집된  세 가지의 데이터셋

*

*

*

<br>

# 논문에 사용된 모델

* 

<br>

# 내용 요약

## 1. Introduction

최근의 트렌드는 각 도메인에 맞게 BERT를 재학습 시키는 것이다.

1. 재학습은 domain-specific

; (ii.) the release of HateBERT, a pre-trained BERT for abusive language phenomena, intended to boost research in this area;(iii.) the release of a large-scale dataset of social media posts in English from communities banned forbeing offensive, abusive, or hateful



<br>

## 2. Related Work

* 

<br>

## 3. Approach





<br>

## 4. Experiments

### 4.1 Data







## 6. Conclusion

<br>

# References



---

바꾼 부분

task_builder

classification_tasks
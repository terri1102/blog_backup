---
layout: default
title: "[NLP] PEGASUS"
date: 2021-08-11
parent: Paper Reviews
grand_parent: Reviews
nav_order: 13
comments: true
use_math: true
---





#  ğŸ—ï¸ ë…¼ë¬¸ í‚¤ì›Œë“œ

`summerization`, `GSG`, 



# ğŸ“‘ ë…¼ë¬¸ ì£¼ì œ





# ğŸ“š ë°ì´í„°ì…‹

* Pre-training dataset
* Fine-tuning dataset 
* 

 <span style="background:#e8f7ff">**Cross-encoder**</span>

# ë‚´ìš© ìš”ì•½

## Abstract

## 3. Pre-training Objectives

GSG ì™€ ë”ë¶ˆì–´ BERTì˜ pre-training objectiveì¸ MLMì„ GSGì™€ í•©ì³ì„œ vs. PLMë§Œ ì‚¬ìš©í•œ ê²ƒì„ ì‚´í´ë³¼ ê²ƒì´ë‹¤.



### 3.1. Gap Sentences Generation(GSG)

ë‹¤ìš´ ìŠ¤íŠ¸ë¦¼ íƒœìŠ¤í¬ì™€ ë¹„ìŠ·í•œ pre-training objectiveë¥¼ ì‚¬ìš©í•˜ë©´ ë” ì¢‹ì€ íš¨ê³¼ë¥¼ ë‚¼ ê²ƒì´ë¼ê³  ìƒê°í•´ì„œ seq2seq self-supervised objtectiveë¥¼ ì´ìš©í•´ì„œ abstractive summaryì—†ì´ pre-train í•˜ì˜€ë‹¤.

GSG: ë¬¸ì„œì˜ ë¬¸ì¥ì„ ê³¨ë¼ì„œ í†µì§¸ë¡œ [mask1] í† í°ìœ¼ë¡œ ë°”ê¿ˆ. ê·¸ë¦¬ê³  gap-sentenceë“¤ì„ concatí•´ì„œ  pseudo-summaryë¡œ ë§Œë“¦. GSR(Gap sentences ratio)ëŠ” mask token ratioë‘ ë¹„ìŠ·í•˜ê²Œ ë§ˆìŠ¤í‚¹ëœ ë¬¸ì¥ ë¹„ìœ¨ì„.

ìš”ì•½ê³¼ ë¹„ìŠ·í•˜ê²Œ ë§Œë“¤ë ¤ê³  ì¤‘ìš”í•œ ë¬¸ì¥ì„ ê³¨ëìŒ. ì´ë¥¼ í†µí•´ì„œ ë§ˆìŠ¤í‚¹ì˜ ì´ì ê³¼ ë‹¤ìš´ìŠ¤íŠ¸ë¦¼ íƒœìŠ¤í¬ì™€ ë¹„ìŠ·í•œ êµ¬ì¡°ë¡œ ë§Œë“¤ë ¤ê³  í–ˆìŒ.

m gap sentence ê³ ë¥´ëŠ” ì „ëµ 3ê°€ì§€ : 

\- Random : uniformí•˜ê²Œ mê°œì˜ ë¬¸ì¥ì„ ëœë¤í•˜ê²Œ ê³ ë¦„ 

\- Lead : ë¬¸ì„œì˜ ì• ë¬¸ì¥ mê°œë¥¼ ê³ ë¦„

\- Principal : ì¤‘ìš”í•œ ìˆœì„œëŒ€ë¡œ top-mê°œì˜ ë¬¸ì¥ì„ ê³ ë¦„. ì¤‘ìš”ë„ë¥¼ í‰ê°€í•˜ê¸° ìœ„í•´ ê° ë¬¸ì¥ê³¼ ë¬¸ì„œ ê°„ì˜ ROUGE1-F1ì„ êµ¬í–ˆìŒ.

Principal ë‚´ì—ì„œì˜ 4ê°€ì§€ ì „ëµ : Ind/Seq, Orig/Uniq



### 3.2 Masked Language Model(MLM)

BERTì—ì„œì²˜ëŸ¼ 15%í† í°ì„ ê³¨ë¼ì„œ ê·¸ ì¤‘ 80%ëŠ” ë§ˆìŠ¤í‚¹ [MASK2] í•˜ê³  10 %ëŠ” ëœë¤ í† í°ìœ¼ë¡œ ë°”ê¾¸ê³ , 10%ëŠ” ì•„ë¬´ ì²˜ë¦¬ë„ ì•ˆ í•˜ì‹œëŠ” ì‹ìœ¼ë¡œ MLMì„ êµ¬í˜„í–ˆë‹¤. í•˜ì§€ë§Œ MLMì€ ë‹¤ìš´ìŠ¤íŠ¸ë¦¼ íƒœìŠ¤í¬ì˜ ì„±ëŠ¥ì„ ê°œì„ í•˜ì§€ ëª»í–ˆê¸° ë•Œë¬¸ì— ìµœì¢… ëª¨ë¸ì¸ pegasus_largeì—ì„œëŠ” ì œì™¸í•˜ì˜€ë‹¤.





## 4. Pre-training Corpus

\- C4 : Colossal and Cleaned version of Common Crawl

\- HugeNews : 1.5B articles from news and news-like websites collected from 2013-2019.



### 6.2. Larger Model Results

### PEGASUS_BASEì™€ PEGASUS_LARGEì˜ Configuration

|                                             | PEGASUS_BASE           | PEGASUS_LARGE             |
| ------------------------------------------- | ---------------------- | ------------------------- |
| Hidden size of attention(H)                 | 768                    | 1024                      |
| Feed-forward layer size (F)                 | 3072                   | 4096                      |
| Number of self-attention (A)                | 12                     | 16                        |
| Number of layers for encoder and decoder(L) | 12                     | 16                        |
| Batch size                                  | 256                    | 8192                      |
| Pre-training steps                          | 500k                   | 500k                      |
| Pre-training objective                      | GSG with MLM, GSG only | GSG(Ind-Orig)             |
| Vocab                                       | BPE, Unigram           | Unigram vocab size of 96k |
| Number of parameters                        |                        | 568M                      |



ëª¨ë¸ì´ ë” ì˜ ë”°ë¼í•  ìˆ˜ ìˆê²Œ í•˜ê¸° ìœ„í•´ ì„ íƒëœ ë¬¸ì¥ë“¤ ì¤‘ 20%ëŠ”  [MASK1]ìœ¼ë¡œ ë°”ê¾¸ì§€ ì•Šê³  ê·¸ëŒ€ë¡œ ë‘ì—ˆë‹¤. ë˜í•œ ìµœì ìœ¼ë¡œ ì°¾ì€ GSRì¸ 30%ì„ ìœ„ì˜ ë³€í™”ì— ë§ì¶”ê¸° ìœ„í•´ 45%ê¹Œì§€ ì˜¬ë ¸ë‹¤.

ê°„ë‹¨í•œ í•˜ì´í¼ íŒŒë¼ë¯¸í„° sweepì„ í†µí•´ì„œ í•™ìŠµë¥ ê³¼ length penaltyë¥¼ ê²°ì •í–ˆë‹¤. 



ëª‡ ê°œì˜ ë°ì´í„°ì…‹ì€ input tokenì˜ ê¸¸ì´ê°€ ìµœëŒ€ ì¸í’‹ ê¸¸ì´ì¸ 512 í† í°ë³´ë‹¤ ê¸¸ì—ˆëŠ”ë°, ì´ë•Œ positional encodingì„ (íŠ¸ëœìŠ¤í¬ë¨¸ì²˜ëŸ¼) sinusoid positional encodingì„ ì‚¬ìš©í•˜ë©´ fine-tuningí•  ë•Œ ë” ê¸´ ê¸¸ì´ë¥¼ ì‚¬ìš©í•´ë„ ì˜ generalizeí•˜ëŠ” ê²ƒì„ í™•ì¸í–ˆë‹¤.



PEGASUS_Largeë¥¼ ì‚¬ìš©í•œ ê²°ê³¼ ì‘ì€ ë°ì´í„°ì…‹ìœ¼ë¡œ fine-tuningì„ ì§„í–‰í•˜ë”ë¼ë„ ì¢‹ì€ ì„±ê³¼ë¥¼ ë³´ì¸ ê²ƒì„ ë³´ë‹ˆ pre-training ê³¼ì •ì—ì„œ ì´ì ì„ ì–»ëŠ”ë‹¤ê³  ë³¼ ìˆ˜ ìˆì—ˆë‹¤. 



### 6.3. Zero and Low-Resource Summerization

í˜„ì‹¤ ì„¸ê³„ì—ì„œëŠ” ìš”ì•½ ë°ì´í„°ì…‹ì´ ì‘ì€ ê²½ìš°ê°€ ë§ë‹¤. ê·¸ë˜ì„œ ê³ ë§™ê²Œë„ ë³¸ ë…¼ë¬¸ì—ì„œëŠ” ì‘ì€ ë°ì´í„°ì…‹ì—ì„œ fine-tunningí•˜ëŠ” ê²½ìš°ì— ëŒ€í•œ ì‹¤í—˜ë„ ì§„í–‰í•˜ì˜€ë‹¤. ë…¼ë¬¸ì—ì„œëŠ” ë°°ì¹˜ ì‚¬ì´ì¦ˆ 256, í•™ìŠµë¥  0.0005ë¡œ 2000 steps í›ˆë ¨í•˜ì˜€ê³  ê°€ì¥ ì¢‹ì€ validation checkpointë¥¼ ê³¨ëë‹¤.

ì‹¤í—˜ ê²°ê³¼ë¥¼ ë³´ë©´ Pegasus_largeëŠ” 100ê°œì˜ ìƒ˜í”Œë¡œ fine-tunningí–ˆì„ ë•Œ Transformer_baseë¥¼ 20k~200kì˜ ë°ì´í„°ì…‹ìœ¼ë¡œ fine-tunningí•œ ê²ƒê³¼ ë¹„ìŠ·í•œ ì„±ëŠ¥ì´ ë‚˜ì™”ë‹¤. 



### 6.4. Qualitative Observations and Human Evaluation

ì´ì „ ì—°êµ¬ ì¤‘ maximum likelihood ë°©ì‹ìœ¼ë¡œ í›ˆë ¨í•œ ëª¨ë¸ë“¤ì´ ë°˜ë³µì ì¸ í…ìŠ¤íŠ¸ë¥¼ ìƒì„±í•´ë‚´ëŠ” ë¬¸ì œë“¤ì„ ê°€ì§€ê³  ìˆì—ˆë˜ ê²ƒê³¼ ë‹¤ë¥´ê²Œ PegasusëŠ” ì´ëŸ° ë¬¸ì œê°€ ì—†ì–´ì„œ ë”°ë¡œ ì´ì— ëŒ€í•œ í•´ê²°ì±…ì„ ê°•êµ¬í•˜ì§€ëŠ” ì•Šì•˜ë‹¤ê³  í•œë‹¤. 

ROUGE í‰ê°€ì§€í‘œê°€ ì™„ë²½í•œ ê±´ ì•„ë‹ˆì§€ë§Œ, Perplexity-optimized models using aggregated ROUGEëŠ” ì¢‹ì€ ëª¨ë¸ì„ ë§Œë“œëŠ” ë° ë„ì›€ì´ ë˜ì—ˆë‹¤. 

ì‚¬ëŒì´ ê²€ìˆ˜í•œ ê²°ê³¼ Pegasus_large ëª¨ë¸ë“¤ì€ XSum, CNN/Daily Mailì—ì„œ reference summaryì™€ ë¹„ìŠ·í•œ í€„ë¦¬í‹°ë¥¼ ë³´ì˜€ì§€ë§Œ Reddit TIFUì˜ ê²½ìš° ë‹¤ì–‘í•œ ë¬¸ì²´ë¡œ ì´ë£¨ì–´ì§„ ë°ì´í„°ì…‹ì´ì–´ì„œ ê·¸ëŸ°ì§€ ê·¸ë ‡ì§€ ëª» í–ˆë‹¤.



## 7. Conclusion

ì´ ë…¼ë¬¸ì—ì„œ seq2seq ëª¨ë¸ì¸ PEGASUSë¥¼ ì†Œê°œí•œë‹¤. PEGASUS ëª¨ë¸ì€ abstractive summary(ìƒì„± ìš”ì•½) íƒœìŠ¤í¬ì— ë§ì¶˜ gap-sentence generationì´ë¼ëŠ” pre-training objectiveì„ ê°€ì§€ê³  í›ˆë ¨ë˜ì—ˆë‹¤. 





# Takeaway

ì‘ì€ ë°ì´í„°ì…‹ìœ¼ë¡œ fine-tunningí•˜ëŠ” ê²ƒì— ëŒ€í•œ ë‚´ìš©ì´ ìˆì–´ì„œ ë„ì›€ì´ ë§ì´ ë˜ì—ˆë‹¤. í•˜ì´í¼ íŒŒë¼ë¯¸í„° ì¡°ì •í•  ë•Œ ì°¸ê³ í•  ë§Œí•  ê²ƒ ê°™ë‹¤.





# Reference


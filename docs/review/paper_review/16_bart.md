---
layout: default
title: "[NLP] BART"
date: 2021-08-11
parent: Paper Reviews
grand_parent: Reviews
nav_order: 16
comments: true
use_math: true
---





#  ğŸ—ï¸ ë…¼ë¬¸ í‚¤ì›Œë“œ





# ğŸ“‘ ë…¼ë¬¸ ì£¼ì œ





# ğŸ“š ë°ì´í„°ì…‹

* 

# ë‚´ìš© ìš”ì•½

## Abstract

BARTë¥¼ ì†Œê°œí•œë‹¤! denoising autoencoder for pretraining sequence-to-sequence models.

Pre-train ë°©ë²•ì€ (1) noising functionìœ¼ë¡œ í…ìŠ¤íŠ¸ë¥¼ ì˜¤ì—¼ì‹œí‚´ (2) ëª¨ë¸ì„ ì´ìš©í•´ì„œ ì˜¤ë¦¬ì§€ë„ í…ìŠ¤íŠ¸ë¥¼ ì˜ˆì¸¡í•¨.

## 2. Model

### 2.1 Architecture

í‘œì¤€ì ì¸ seq2seq íŠ¸ëœìŠ¤í¬ë¨¸ ì•„í‚¤í…ì²˜ë¥¼ ì‚¬ìš©í•˜ê³ , GPTì²˜ëŸ¼ GeLUë¥¼ ì‚¬ìš©. ì •ê·œë¶„í¬(0, 0.02) ì‚¬ìš©

Bart_baseëŠ” ì¸ì½”ë”ì™€ ë””ì½”ë”ì— ê°ê° 6ê°œì˜ ë ˆì´ì–´ ì‚¬ìš©.

BERTì™€ ì°¨ì´ : 

BARTëŠ” ë””ì½”ë”ì˜ ë ˆì´ì–´ ë§ˆë‹¤ ì¸ì½”ë”ì˜ ë§ˆì§€ë§‰ hiddenê³¼ cross-attention ìˆ˜í–‰

BERTì—ì„œëŠ” ë‹¨ì–´ ì˜ˆì¸¡ ì „ì— ì‚¬ìš©í•œ feed-forward networkë¥¼ BARTì—ì„œëŠ” ì‚­ì œ



### 2.2 Pre-training BART

BARTëŠ” ë””ì½”ë”ì˜ outputê³¼ original documentì˜ cross entropy lossë¥¼ ì¤„ì´ëŠ” ë°©ì‹ìœ¼ë¡œ í•™ìŠµí•œë‹¤.



## Noise ë°©ë²•

### 1. Token Masking

BERTì²˜ëŸ¼ ì„ì˜ì˜ í† í°ì„ [MASK]ë¡œ êµì²´

[MASK] í† í°ì´ ë¬´ì—‡ì¸ì§€ ì˜ˆì¸¡



### 2. Token Deletion

ì„ì˜ì˜ í† í°ì„ ì‚­ì œ

ì‚­ì œí•œ í† í°ì˜ ìœ„ì¹˜ ì˜ˆì¸¡



### 3. Text infilling

í‘¸ì•„ì†¡ ë¶„í¬(lambda=3)ì—ì„œ span lengthë¥¼ ë½‘ê³ ,ê° spanì€ í•˜ë‚˜ì˜ [MASK] í† í°ìœ¼ë¡œ ëŒ€ì²´

spanì´ 0ì´ë©´ í† í°ì„ ì§€ìš°ì§€ ì•Šê³  [MASK] í† í° ë„£ìŒ.

Text infillingì€ ëª¨ë¸ì´ ëª‡ ê°œì˜ tokenì´ spanì—ì„œ ì‚¬ë¼ì¡ŒëŠ”ì§€ ì˜ˆì¸¡í•˜ê²Œ í•¨. 



SpanBERTì™€ì˜ ì°¨ì´: SpanBertëŠ” clamped geometric ë¶„í¬ì—ì„œ ìƒ˜í”Œë§ì„ í•˜ê³  ê° ìŠ¤íŒ¬ì„ ëŒ€ì²´í•˜ëŠ” í† í° ê°œìˆ˜ì™€ ë™ì¼í•œ [MASK] ê°œìˆ˜ë¡œ ë§ˆìŠ¤í‚¹í•¨. Text infillingì€ ì—¬ëŸ¬ í† í°ì„ í•˜ë‚˜ì˜ [Mask]ë¡œ ë§ˆìŠ¤í‚¹í•´ì„œ ëª‡ ê°œì˜ í† í°ì´ ëŒ€ì²´ë˜ì—ˆëŠ”ì§€ë„ í•™ìŠµí•˜ê²Œ í•¨.



### 4. Sentence Permutation

ë¬¸ì¥ì˜ ìˆœì„œë¥¼ ëœë¤ìœ¼ë¡œ ì„ìŒ



### 5. Document Rotation

í•˜ë‚˜ì˜ tokenì„ uniformí•˜ê²Œ ë½‘ì€ í›„ ê·¸ í† í°ì„ ì‹œì‘ì ìœ¼ë¡œ íšŒì í•¨

ëª¨ë¸ì´ ë¬¸ì„œì˜ ì‹œì‘ì ì„ ì°¾ë„ë¡ í•™ìŠµ



 <span style="background:#e8f7ff">**Text infilling**</span> ì´ ê°€ì¥ íš¨ê³¼ì ì´ì—ˆìŒ





## 3. Fine-tuning BART

### 3.3 Sequence generation task

ìƒì„± QAë‚˜ ìš”ì•½ íƒœìŠ¤í¬ì˜ ê²½ìš° inputì„ ë³µì œí•´ì„œ ì´ë¥¼ ì¡°ì‘í•œ í›„ ì˜ˆì¸¡í•˜ê²Œ ë˜ëŠ”ë°, denoising taskì™€ ë§¤ìš° í¡ì‚¬í•˜ë‹¤. ì¸ì½”ë” inputì€ input sequenceì´ë©° ë””ì½”ë”ëŠ” auto-regressiveí•˜ê²Œ outputì„ ìƒì„±í•œë‹¤. 



## 4. Comparing Pre-training Objectives

BART base ëª¨ë¸(ì¸ì½”ë”, ë””ì½”ë” ê°ê° 6ë ˆì´ì–´, íˆë“  ì‚¬ì´ì¦ˆ 768)



### 4.1 Comparison Objective

## 5. Large-scale Pre-training Experiments

### 5.1 Experimental Setup

ì¸ì½”ë”, ë””ì½”ë” ê°ê° 12 ë ˆì´ì–´ì´ê³  íˆë“  ì‚¬ì´ì¦ˆê°€ 1024ì¸ Large ëª¨ë¸ì„ ì‚¬ìš©í–ˆë‹¤.

RoBERTaì™€ ê°™ì€ ì¡°ê±´ìœ¼ë¡œ ë§ì¶”ê¸° ìœ„í•´ ë°°ì¹˜ ì‚¬ì´ì¦ˆëŠ” 8000, 500000 ìŠ¤í… ë™ì•ˆ í›ˆë ¨í–ˆë‹¤. GPT2ì™€ ê°™ì€ byte-pair ë°©ì‹ìœ¼ë¡œ í† í¬ë‚˜ì´ì§•í–ˆë‹¤. Text infilling ë°©ë²•ê³¼ sentence permutationì„ ì‚¬ìš©í–ˆëŠ”ë° ê° documentì˜ 30%ë¥¼ ë§ˆìŠ¤í‚¹í–ˆê³  ëª¨ë“  ë¬¸ì¥ì„ permuteí–ˆë‹¤. íŠ¸ë ˆì´ë‹ ìŠ¤í…ì˜ ë§ˆì§€ë§‰ 10%ì—ì„œëŠ” dropoutë¥¼ ë¹„í™œì„±í™”í•´ì„œ ë°ì´í„°ì— fití•˜ê²Œ í›ˆë ¨í•  ìˆ˜ ìˆê²Œ í–ˆë‹¤. 160gbì˜ ë‰´ìŠ¤, ì±…, ì´ì•¼ê¸°, ì›¹ í…ìŠ¤íŠ¸ë¥¼ ì´ìš©í•´ì„œ í›ˆë ¨í–ˆë‹¤. 

### 5.3 Generation Tasks

Bart largeëŠ” í‘œì¤€ì ì¸ Seq2seq ëª¨ë¸ë¡œì„œ fine-tuneë˜ì—ˆë‹¤. ì´ ê³¼ì •ì—ì„œ label smoothed cross entropy lossë¥¼ ì‚¬ìš©í–ˆê³ , smoothing parameterëŠ” 0.1ë¡œ ì£¼ì—ˆë‹¤. ìƒì„± ê³¼ì •ì—ì„œ beam sizeëŠ” 5ë¡œ ë‘ê³  beam search ê³¼ì •ì—ì„œ ì¤‘ë³µë˜ëŠ” trigramì„ ì œê±°í–ˆë‹¤. validation setìœ¼ë¡œ ì˜ˆì¸¡ì‹œ min0len, max-len, length penaltyë¥¼ ì„¤ì •í–ˆë‹¤.



# Takeaway

Pre-training ë‹¨ê³„ì—ì„œ ì–´ë ¤ìš´ íƒœìŠ¤í¬ë¥¼ ì£¼ë©´ ëª¨ë¸ì´ ë” ì˜ ë°°ìš´ë‹¤.





# Reference

[[Paper Review] Transformer to T5 (XLNet, RoBERTa, MASS, BART, MT-DNN,T5)](https://www.youtube.com/watch?v=v7diENO2mEA&list=LL&index=2&t=5s)

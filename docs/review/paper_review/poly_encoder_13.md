---
layout: default
title: "[NLP] Poly-encoders: architectures and pre-training
strategies for fast and accurate multi-sentence scoring ë¦¬ë·°"
date: 2021-08-11
parent: Paper Reviews
grand_parent: Reviews
nav_order: 13
comments: true
use_math: true
---





# :key:ë…¼ë¬¸ í‚¤ì›Œë“œ

`encoder`, `poly-encoder`, `bi-encoder`, `cross-encoder` 



# :bookmark_tabs: ë…¼ë¬¸ ì£¼ì œ

Facebook AI ResearchíŒ€ì—ì„œ Bi-encoderë³´ë‹¤ ì„±ëŠ¥ì„ ê°œì„ í•˜ê³ , Cross-encoderë³´ë‹¤ ì˜ˆì¸¡ ì‹œê°„ì„ ê°œì„ í•œ Poly-encoder ì œì•ˆ



# ğŸ“š ë°ì´í„°ì…‹

* Reddit

* Wiki + Toronto books corpus

  

# ë‚´ìš© ìš”ì•½

## Abstract

Pairwise comparisonì´ë‚˜ ì£¼ì–´ì§„ ì¸í’‹ì„ ê´€ë ¨ ë¼ë²¨ê³¼ ë§¤ì¹­í•˜ëŠ” í…ŒìŠ¤í¬ì— íŠ¸ëœìŠ¤í¬ë¨¸ ê¸°ë°˜ ì•„í‚¤í…ì²˜ë¥¼ ì‚¬ìš©í•œ 2ê°€ì§€ ë°©ë²•ì´ ì£¼ë¡œ ì‚¬ìš©ë˜ì—ˆë‹¤. ê·¸ ì¤‘ <span style="background:#e8f7ff">**Cross-encoder**</span>ëŠ” ì‹œí€€ìŠ¤ ìŒë³„ë¡œ self attentionì„ ìˆ˜í–‰í•˜ëŠ” ë°©ë²•ìœ¼ë¡œ ì„±ëŠ¥ì€ ì¢‹ì§€ë§Œ Inferenceê°€ ëŠë¦¬ë‹¤ëŠ” ë‹¨ì ì´ ìˆì—ˆê³ , <span style="background:#fff28c">**Bi-encoder**</span>ëŠ” ì‹œí€€ìŠ¤ë¥¼ ê°ê° ì¸ì½”ë”©í•˜ì—¬ InferenceëŠ” ë¹ ë¥´ì§€ë§Œ, Cross-encoderë³´ë‹¤ ì„±ëŠ¥ì´ ë–¨ì–´ì¡Œì—ˆë‹¤. ë³¸ ë…¼ë¬¸ì—ì„œ ì œì•ˆí•˜ëŠ” <span style="background:#ffe4de">**Poly-encoder**</span>ëŠ” í† í° ë ˆë²¨ì´ ì•„ë‹Œ ì „ì—­ì ìœ¼ë¡œ self attention í”¼ì²˜ë¥¼ ë°°ìš°ê¸° ë•Œë¬¸ì— cross-encoderë³´ë‹¤ ë¹ ë¥´ê³  Bi-encoderë³´ë‹¤ ì •í™•í•˜ë‹¤. ê·¸ ì™¸ ë°œê²¬ìœ¼ë¡œëŠ” ì‚¬ì „í•™ìŠµ(pre-training) ë‹¨ê³„ì—ì„œë„ ë‹¤ìš´ìŠ¤íŠ¸ë¦¼ í…ŒìŠ¤íŠ¸ì™€ ë¹„ìŠ·í•œ ë°ì´í„°ì…‹ìœ¼ë¡œ pre-trainingí•  ë•Œ ì„±ëŠ¥ì´ ì¢‹ë‹¤ëŠ” ì ì„ ì†Œê°œí•œë‹¤.



* **Alias ì •ë¦¬:** ë…¼ë¬¸ì—ì„œ Input contextì™€ Candidate labelì„ ê°€ë¦¬í‚¤ëŠ” ë‹¨ì–´ë“¤ì´ ë‹¤ì–‘í•˜ê²Œ ì‚¬ìš©ë˜ê¸°ì— ì •ë¦¬í•´ë‘ê² ë‹¤.
  
  * Input context (ì¸í’‹) : Input, Context
  
  * Candidate label (ë¼ë²¨ í›„ë³´) : candidate, label, response
  
    

## 1. Introduction

**Multi-sentence scoring** : Input contextê°€ ì£¼ì–´ì¡Œì„ ë•Œ ì‘ë‹µ í›„ë³´ë“¤ì˜ ì ìˆ˜ë¥¼ ë§¤ê¸°ëŠ” ê²ƒì´ë¡œ Retrieval and dialogue taskì— ë§ì´ ì‚¬ìš©ëœë‹¤. ì´ Multi-sentence scoring ë¶„ì•¼ì—ì„œ ìµœê·¼ SOTA ëª¨ë¸ë“¤ì€ ì‚¬ì „í•™ìŠµëœ BERTëª¨ë¸ë“¤ì„ ì‚¬ìš©í•œ Bi-encoderë‚˜ Cross-encoderê°€ ë§ì´ ì´ìš©ëœë‹¤. 



<span style="background:#e8f7ff">**Cross-encoder**</span>ëŠ” full (cross) self-attentionì„ ì£¼ì–´ì§„ ì¸í’‹ê³¼ ë¼ë²¨ í›„ë³´ë³„ ë¡œ ì§„í–‰í•˜ê¸° ë•Œë¬¸ì— ì„±ëŠ¥ì´ ì¢‹ì§€ë§Œ ëŠë¦¬ë‹¤.
<span style="background:#fff28c">**Bi-encoder**</span>ëŠ” ì¸í’‹ê³¼ ë¼ë²¨ í›„ë³´ë¥¼ ë”°ë¡œ self-attentionì„ ì§„í–‰í•œ í›„ ì´ë¥¼ í•©ì³ì„œ final representationì„ ë§Œë“ ë‹¤. ì¸ì½”ë”©ëœ ì‘ë‹µ í›„ë³´ë“¤ì„ ìºì‹±í•´ì„œ ì´ë¥¼ ì¸í’‹ì— ì¡°í•©í•  ë•Œ ì¬ì‚¬ìš©ê°€ëŠ¥í•˜ê¸°ì— ë¹ ë¥´ë‹¤.

ë³¸ ë…¼ë¬¸ì—ì„œ ì†Œê°œí•˜ëŠ” <span style="background:#ffe4de">**Poly-encoder**</span>ëŠ” self-attentionì„ ìˆ˜í–‰í•˜ê¸° ìœ„í•´ ì „ì—­ì ì¸ íŠ¹ì„±ì„ í‘œí˜„í•˜ëŠ” ì¶”ê°€ë¡œ í•™ìŠµí•œ attention ë§¤ì»¤ë‹ˆì¦˜ì„ ì‚¬ìš©í•˜ì—¬ Bi-encoderë³´ë‹¤ ì¢‹ì€ ì„±ëŠ¥ì„ ë³´ì—¬ì£¼ì—ˆë‹¤.

ìœ„ì˜ ëª¨ë¸ë“¤ì„ ë¹„êµí•˜ê¸° ìœ„í•´ì„œ Dialogue and information retrieval(IR)ì—ì„œ ì‚¬ìš©ë˜ëŠ” 4ê°€ì§€ì˜ ë°ì´í„°ì…‹ì„ í†µí•´ ëª¨ë¸ì„ ê²€ì¦í•˜ëŠ”ë° Redditê³¼ Wikipedia/Toronto Booksë¡œ ê°ê° ì‚¬ì „í•™ìŠµì‹œì¼œì„œ ë‹¤ìš´ ìŠ¤íŠ¸ë¦¼ê³¼ ë¹„ìŠ·í•œ ë°ì´í„°ë¡œ ì‚¬ì „í•™ìŠµí–ˆì„ ë•Œì˜ ê²°ê³¼ë¥¼ ë¹„êµí•œë‹¤. (wiki ë°ì´í„°ë¡œ í•™ìŠµí•œ BERTì™€ ë‹¤ë¥´ê²Œ ëŒ€í™”í˜• ë°ì´í„°ì¸ ë ˆë”§ ë°ì´í„° ì‚¬ìš©í•´ ëŒ€í™”í˜• ì‹œí€€ìŠ¤ ìŠ¤ì½”ë§ë‹¤ìš´ìŠ¤íŠ¸ë¦¼ í…ŒìŠ¤í¬ì— ì ìš©í•¨) ìš°ë¦¬ì˜ ëª¨ë¸ ì•„í‚¤í…ì²˜ì™€ ìƒˆë¡œìš´ ì‚¬ì „í•™ìŠµì„ í†µí•´ì„œ ìƒˆë¡œìš´ SOTAë¥¼ ë‹¬ì„±í–ˆë‹¤. 

## 2. Related Work

ë¨¸ì‹ ëŸ¬ë‹ì— ìˆì–´ì„œ ë‹¤ì–‘í•œ í›„ë³´ë“¤ì„ ìŠ¤ì½”ì–´ë§í•˜ëŠ” ê²ƒì€ ì•„ì£¼ í´ë˜ì‹í•œ ë¬¸ì œì´ë‹¤. ì´ë¥¼ ì´ì‚°ì ì¸ multi-class ë¶„ë¥˜ë¼ëŠ” íŠ¹ìˆ˜í•œ ê²½ìš°ë¡œ ë³¼ ìˆ˜ë„ ìˆì§€ë§Œ ì´ë¥¼ ì¼ë°˜í™”í•˜ë©´ ì‘ë‹µ í›„ë³´ë“¤ì€ í´ë˜ìŠ¤ê°€ ì•„ë‹Œ êµ¬ì¡°ì²´ì´ë‹¤. ì´ ì—°êµ¬ì—ì„œ ìš°ë¦¬ëŠ” ì¸í’‹ê³¼ í›„ë³´ ë¼ë²¨(candidate label)ì„ í…ìŠ¤íŠ¸ ì‹œí€€ìŠ¤ë¡œ ê³ ë ¤í•  ê²ƒì´ë‹¤.

<span style="background:#fff28c">**Bi-encoder**</span> 
Inputê³¼ candidateë¥¼ ê°ê° common feature spaceì— ë§¤í•‘í•œ í›„ì— ì´ ë‘˜ì˜ ìœ ì‚¬ì„±ì„ ë‚´ì ì´ë‚˜ ì½”ì‚¬ì¸ ìœ ì‚¬ë„ë¥¼ í†µí•´ì„œ êµ¬í•œë‹¤. ì¦‰ ì„ë² ë”© ë²¡í„°ê°€ Inputê³¼ candidateì—ì„œ ê°ê° ë‚˜ì˜¨ë‹¤. ë…¼ë¬¸ì—ì„œ Next utterance prediction taskë¥¼  (NSPë‘ ê°™ì€ ê±´ê°€????) ìœ„í•œ Bi-encoder ì ‘ê·¼ìœ¼ë¡œ ë©”ëª¨ë¦¬ ë„¤íŠ¸ì›Œí¬, íŠ¸ëœìŠ¤í¬ë¨¸ ë©”ëª¨ë¦¬ ë„¤íŠ¸ì›Œí¬, LSTM, CNN ë“±ì„ ê³ ë ¤í–ˆì—ˆë‹¤.(ê·¼ë° ì•ˆ ì“´ ê±°ì§€? BERTê¸°ë°˜ ì¸ì½”ë” ì“´ ê±°ì§€??)  Bi-encoderì˜ ì¥ì ì€ í¬ê³  ê³ ì •ëœ ì‚¬ì´ì¦ˆì˜ candidate setì„ ìºì‹±í•œ í›„ì— ì‚¬ìš©í•˜ì—¬, inputê³¼ ë…ë¦½ì ì´ê¸°ì— evaluation ê³¼ì •ì—ì„œ ì—°ì‚° íš¨ìœ¨ì ì´ë‹¤.

<span style="background:#e8f7ff">**Cross-encoder**</span>
Inputê³¼ candidateë¥¼ concatí•œ í›„ì— self-attentionì„ ì ìš©í•œë‹¤. ë”°ë¼ì„œ ë‘ ë¬¸ì¥ì˜ ì„ë² ë”© ê°„ì˜ ê´€ê³„(ìœ ì‚¬ë„)ë¥¼ êµ¬í•˜ì§€ ì•Šê³  ë‘ ë¬¸ì¥ì„ í•©ì¹œ ê²ƒì´ í™œì„±í™” í•¨ìˆ˜ë¡œ ë“¤ì–´ê°€ì„œ í•˜ë‚˜ì˜ ì„ë² ë”©ì„ ì–»ê²Œ ëœë‹¤. ì´ ê²½ìš° candidateë“¤ì˜ ëª¨ë“  ë‹¨ì–´ì™€ inputì˜ ëª¨ë“  ë‹¨ì–´ ê°„ì˜ ìƒí˜¸ì‘ìš©ì„ ì•Œ ìˆ˜ ìˆì–´ì„œ Input sequenceì™€ candidate sequenceë“¤ì˜ ê´€ê³„ë¥¼ ì˜ ë‚˜íƒ€ë‚¼ ìˆ˜ ìˆë‹¤. 

## 3. Tasks

**ConvAI2 í…ŒìŠ¤í¬ ì„¤ëª…**

![convai2](https://paperswithcode.com/media/datasets/ConvAI2-0000003063-95759eda.jpg)20ê°œì˜ ì„ íƒì§€ ì¤‘ì— ê°€ì¥ ì í•©í•œ ì‘ë‹µ í›„ë³´ë¥¼ ì„ íƒí•´ì•¼í•¨. í˜ë¥´ì†Œë‚˜
\<convai2> ë°ì´í„°ì…‹ ì˜ˆì‹œ ë³´ì—¬ì£¼ê¸°



StarSpace ì„¤ëª… í•„ìš”í•¨

**Dataset ì„¤ëª…**

![poly2](https://github.com/terri1102/terri1102.github.io/blob/master/assets/images/review/polyencoder2.jpg?raw=true)

## 4. Methods



### 4.1 Transformers and Pre-traininig Strategies

* **Transformers**

ë¹„êµë¥¼ ìœ„í•´ì„œ 2ê°€ì§€ ë°ì´í„°ì…‹ì„ ì´ìš©í•´ íŠ¸ëœìŠ¤í¬ë¨¸ë¥¼ í›ˆë ¨í•¨ 
1)BERT-baseì™€ ë¹„ìŠ·í•˜ê²Œ ìœ„í‚¤í”¼ë””ì•„ì™€ í† ë¡ í†  ë¶ìŠ¤ ì½”í¼ìŠ¤ì—ì„œ ì¶”ì¶œí•œ input, labelì„ ìŠ¤í˜ì…œ í† í°[S]ë¡œ ê°ìŒˆ
2 Redditì˜ ê²½ìš° inputì€ contextì´ê³  labelì€ next utterance

ìœ„í‚¤í”¼ë””ì•„, í† ë¡ í†  : input: ë¬¸ì¥ í•˜ë‚˜. label: ë‹¤ìŒ ë¬¸ì¥
ê° ì¸í’‹ í† í°ì€ 3ê°€ì§€ ì„ë² ë”©ì˜ í•©! (token embedding, position embedding, segment embedding) -> bertì™€ ë™ì¼í•œë“¯

* **Input Representation**

pre-training inputì€ inputê³¼ labelì˜ concatí•œ ê²ƒ. ëª¨ë‘ ìŠ¤í˜ì…œ í† í° [S]ë¡œ ë‘˜ëŸ¬ì‹¸ì—¬ ìˆìŒ(Lample&Conneau,2019) ì°¸ì¡°.

* **Pre-training Procedure**
* **Fine-tuning**

### 4.2 Bi-encoder

<img src="https://github.com/terri1102/terri1102.github.io/blob/master/assets/images/review/bi-encoder.jpg?raw=true" alt="bi-encoder" style="zoom:100%;" class="center"  />



ì´ì œ ìœ„ì˜ 4.1ì—ì„œ ì‚¬ì „í•™ìŠµí•œ ì¸ì½”ë”ë¥¼ Fine-tuning ê³¼ì •ì— ëŒ€í•œ ì„¤ëª…ì´ë‹¤. 
Bi-encoder ì„¤ëª…
$$
y_{ct xt} = red(T_1(ctxt)) \quad y_{cand}=red(T_2(cand))
$$

$$
y_{ctxt}
$$

\(y_{ctxt}\)

ëŠ” Inputì„ ì…ë ¥ë°›ì•„ì„œ ë²¡í„°ë¡œ ì¸ì½”ë”©ëœ í›„(T1(ctxt))ì— red(Â·) í•¨ìˆ˜ë¥¼ ê±°ì³ì„œ í•˜ë‚˜ì˜ ë²¡í„°ë¡œ ì¶•ì†Œëœ **ì„ë² ë”© ë²¡í„°**ì´ë‹¤. (ê·¸ë¦¼ì—ì„œ ctxt Emb)

* T(x) = h1,...,h_n : íŠ¸ëœìŠ¤í¬ë¨¸ì˜ output (out1 == h1 == [S]ì˜ ì„ë² ë”©ê°’)

* red(Â·) : ë²¡í„°ë“¤ì˜ ì‹œí€€ìŠ¤ë¥¼ í•˜ë‚˜ì˜ ë²¡í„°ë¡œ ì¶•ì†Œì‹œí‚¤ëŠ”(reduce) í•¨ìˆ˜

  redë¡œ ì‚¬ìš©í•  í•¨ìˆ˜ë¡œ 

  1. íŠ¸ëœìŠ¤í¬ë¨¸ì˜ ì²« ë²ˆì§¸ outputì„ ì·¨í•˜ëŠ” í•¨ìˆ˜

  2) ëª¨ë“  outputì˜ í‰ê· ê°’ì„ ì·¨í•˜ëŠ” í•¨ìˆ˜
  3) ì²« mê°œì˜ outputì˜ í‰ê· ê°’ì„ ì·¨í•˜ëŠ” í•¨ìˆ˜ (m<N)

  ë¥¼ ê³ ë ¤í–ˆì—ˆëŠ”ë°, 1ë²ˆ í•¨ìˆ˜ ì‚¬ìš©í–ˆë‹¤. ë‹¤ë¥¸ í•¨ìˆ˜ë¥¼ ì‚¬ìš©í–ˆì„ ë•Œì˜ ê²°ê³¼ëŠ” Appendixì—ì„œ ë³¼ ìˆ˜ ìˆë‹¤.

* Context encoder : Bertì™€ ë™ì¼í•˜ê²Œ 3ê°€ì§€ì˜ ê°’ì„ ë”í•´ì„œ ì„ë² ë”©ì„ êµ¬í•¨. ì´ë•Œ inputê³¼ candidateëŠ” ê°ê° ì¸ì½”ë”©ë˜ê¸° ë•Œë¬¸ì— segment tokensì€ ëª¨ë‘ 0ì´ë‹¤. ì‚¬ì „í•™ìŠµì—ì„œ ë¹„ìŠ·í•œ í™˜ê²½ì„ ëª¨ë°©í•˜ê¸° ìœ„í•´ inputê³¼ candidateëŠ” ëª¨ë‘ ìŠ¤í˜ì…œ í† í° [S]ë¡œ ë‘˜ëŸ¬ ì‹¸ì¸ë‹¤. 

Bi-encoderëŠ” inputê³¼ candidateë¥¼ ê°ê° ì¸ì½”ë”©í•˜ê¸° ë•Œë¬¸ì— candidateì˜ representationì€ ìºì‹±í•œ í›„ì— inferenceí•  ë•Œ ê°€ì ¸ì™€ì„œ ì‚¬ìš©í•  ìˆ˜ ìˆë‹¤. 



ëª¨ë¸ 2ê°œë¥¼ ë§Œë“¤ì–´ì„œ ì‹¤í—˜ í›„ì— ë” ë‚˜ì€ ëª¨ë¸ ì‚¬ìš©í•  ê²ƒ. 
T1, T2 ëª¨
ë‘ pre-trainedëœ íŠ¸ëœìŠ¤í¬ë¨¸. ë‘˜ ë‹¤ ê°™ì€ ê°€ì¤‘ì¹˜ë¡œ ì‹œì‘í•˜ì§€ë§Œ íŒŒì¸íŠœë‹ ê³¼ì •ë™ì•ˆ ë‹¬ë¼ì§ˆ ë“¯. T(x) = h1, h2...,hnì€ íŠ¸ëœìŠ¤í¬ë¨¸ì˜ outputì´ê³  red()ëŠ” ë²¡í„°ë“¤ì˜ ì‹œí€€ìŠ¤ë¥¼ ë²¡í„° í•˜ë‚˜ë¡œ ì¤„ì´ëŠ” í•¨ìˆ˜
inputê³¼ labelì´ ê°ê° ì¸ì½”ë”©ë˜ê¸° ë•Œë¬¸ì— segment tokenì€ ë‘˜ ë‹¤ 0ì„
pre-training ê³¼ì •ì„ ëª¨ë°©í•˜ê¸° ìœ„í•´ inputê³¼ labelì€ ê°ê° ìŠ¤í˜ì…œ í† í° [s]ë¡œ ë‘˜ëŸ¬ ì‹¸ì„

* **Scoring**

cand_iì˜ ì ìˆ˜ëŠ” 
$$
s(ctxt,cand_i)=y_{ctxt y}Â· y_{cand_i}
$$
 inputì˜ ì„ë² ë”©ê³¼ candidateì˜ ì„ë² ë”©ì„ ë‚´ì í•œ ê°’ì´ë‹¤. ëª¨ë¸ì€ cross-entropy lossë¥¼ ìµœì†Œí™”í•˜ëŠ” ë°©ì‹ìœ¼ë¡œ í›ˆë ¨ë˜ì—ˆë‹¤. 



* **Inference speed**

  

### 4.3 Cross-encoder

<img src="https://github.com/terri1102/terri1102.github.io/blob/master/assets/images/review/cross-encoder.jpg?raw=true" alt="crossencoder" style="zoom:100%;" class="center" />
$$
y_{ctxt,cand} = h_1 = first(T(ctxt,cand))
$$


* Scoring

$$
s(ctxt, cand_i) = y_{ctxt,cand_i}W
$$



### 4.4 Poly-encoder

<img src="https://github.com/terri1102/terri1102.github.io/blob/master/assets/images/review/poly-encoder.jpg?raw=true" alt="polyencoder" style="zoom:100%;" class="center" />

## 5. Experiments



### 5.1 Bi-encoders and Cross-encoders

![polyencoder3](https://github.com/terri1102/terri1102.github.io/blob/master/assets/images/review/polyencoder3.jpg?raw=true)

![polyencoder4]

![polyencoder5]

### 5.2 Poly-encoders

![polyencoder6]

### 5.3 Domain-specific Pre-training



### 5.4 Inference Speed



## 6. Conclusion



## A Training Time



## B Reduction layer



## C Alternative Choices for Context Vectors





# :meat_on_bone: â€‹Takeaway



# Reference

**Cross-encoder**

Thomas Wolf, Victor Sanh, Julien Chaumond, and Clement Delangue. Transfertransfo: A
transfer learning approach for neural network based conversational agents. arXiv preprint
arXiv:1901.08149, 2019.

**Bi encoder**

Pierre-Emmanuel MazarÂ´e, Samuel Humeau, Martin Raison, and Antoine Bordes. Training millions
of personalized dialogue agents. In EMNLP, 2018.

SBERT Bi-Encoder 

https://towardsdatascience.com/advance-nlp-model-via-transferring-knowledge-from-cross-encoders-to-bi-encoders-3e0fc564f554

Augmented SBERT

https://karter.io/sentence-bert-augmented-sbert

---
layout: default
title: "[NLP] Exploring the Limits of Transfer Learning with a unified Text-to_text Transformer"
date: 2021-08-11
parent: Paper Reviews
grand_parent: Reviews
nav_order: 17
comments: true
use_math: true
---



## Abstract

* MT-DNN처럼 다양한 문제를 해결. But text-to-text로 해결하는 것이 확률 분포로 해결하는 mT-dnn과 다른 점.

* Text-to-text 방법과 Colossal Clean Crawled Corpus를 결합하여 많은 벤치마크에서 SOTA를 달성했음

* 데이터가 중요하다



## Introduction

동일한 모델 objective
$$
P(y_t | y_1, \cdots, y_{t-1},x)
$$
x는 인코더 토큰. y는 디코더 토큰들



* 11B 파라미터를 사용해 SOTA 달성
* C4 데이터 공개



## 2.1 Model

* Transformer

* Self-attention

* Layer normalization의 간단한 형태 사용. bias 사용하지 않음

* $$
  원래 \; layer \; norm: (\frac{x-M}{\sigma})W + b
  $$

  인데 M과 bias를 뺐다.

* 

* Position embedding: 레이어별로 공유된다. 근데 헤드별로 다른 position embedding있음. 최대 128 토큰 길이. relative position 

![image-20211019220957400](C:\Users\Boyoon Jang\AppData\Roaming\Typora\typora-user-images\image-20211019220957400.png)

개수를 로그스케일로 증가?시켰다. 대각선 위로 15, 아래로 16 등...32개의 값만 가지게 했다.

## 2.2 Data

750G



## 2.4 Setup-i/o Format

Pre-training과 fine-tuning의 목적함수가 동일함
$$
P(y_t|y_1,...,y_{t-1}  ,x)
$$


Teacher forcing : 정답도 넣어줌



STS-B는 regression task로 1에서 5사이의 similarity score 예측. 근데 점수를 0.2 단위로 증가하도록 해서 예측하기에 21가지 classification 문제.

WNLI, WSC, DPR: ""로 둘러싼 단어를 예측하게 만듦



## Experiments-Baseline

d_model 768

encoder, decoder 각각 12 blocks

BERT base의 두 배

Training: teacher forcing을 이용한 표준 maximum likelihood 학습

Adafactor optimizer

inverse square root scheduler 사용

평가는 각 task의 dev 셋으로 했다고 함. 학습은 데이터 섞어서 하고 평가는 따로따로 한 후 가장 좋은 체크포인트 사용



* unsupervised obejctive

  causal language model 보다는 masked language model 성능이 좋음

  target은 동일한 sentinel token 



baseline performance

* base line을 각각 10회 train 후 downstream task score 평균 구함



## Experiments - Architecture



* 세 개의 unsupervised objective 비교

1. Prefix-language modeling
2. Bert style : auto-encoder 방식
3. Deshuffling



Replace corrupted spans: 15%를 마스킹. 개별 토큰이 아니고 span을 mask

corruption rate를 변경해봤는데 15%가 가장 좋았다. 15는 매직 넘버임ㅋㅋ

corrupting spans: 2,3,5,10-> 10인 경우 성능이 감소함. span 길이 3이 NMT 제외하고 성능이 좋음

옵티멀한 해를 찾아간 것.



### Pre-training Data set

C4가 가장 성능이 좋음 . 정제되고 큰 데이터가 좋다

제한된 범위의 데이터가 어떤 경우는 더 좋음



반복: epoch -1



### Fine tuning Methods

adaptive layer : 각 레이어의 마지막 ff 뒤에 새로운 ff 추가. 근데 기존 attention + ff는 freeze함

gradual unfreezing : 첫 번째 레이어부터 마지막 레이어까지 모델을 순차적으로 학습시킴



### Multi-task learning

다양한 태스크의 데이터를 섞어 놓고 학습하는데 어떤 데이터를 섞으면 떨어지기도 함. negative effect 주는 태스크, positive effect 주는 태스크 . 태스크 간 상성을 잘 봐야 함

* 태스크별 데이터 사이즈를 비슷하게 하거나 temperature로 조정해봄.
* 멀티 태스크 러닝과 파인튜닝을 조합함. 
* pre-training할 때 (unsupervised + supervised) 믹싱
* 관련 데이터로 파인튜닝함. 
* 근데 멀티태스크로 파인튜닝한 거랑 unsupervised 프리트레이닝 한 거랑 비슷하더라~



### Scaling

모델이 커지는 거나 training step이 높아지는 것 모두 성능 개선에 효과 있다. 



### Putting it all together

Objective 

iid(independent and identically distributed) denoising, span length 3, corrupt 15%

longer traininig: 1M train step, batch size 2048

beam search

adafactor optimizer

​		- pre train: inverse square root learning rate schedule

​		-finetuing: constant learning rate


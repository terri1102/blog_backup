---
layout: default
title: 자연어 처리 3부: 언어 모델 1: RNN, LSTM, GRU
date: 2021-04-17
categories:
  - Deep_Learning
tags:
  - Deep_Learing
comments: true
nav_order: 6
parent: Deep Learning
---

d

https://www.youtube.com/watch?v=WsQLdu2JMgI

NLP 3부 언어 모델 편입니다. 여기서는 언어 모델로 RNN 계열 모델(RNN, LSTM, GRU)을 설명합니다.

# 언어 모델(Language Model)

언어 모델은 단어에 나열 확률을 부여해서 특정한 단어의 시퀀스에 대해 그 시퀀스가 일어날 가능성이 어느 정도인지를 확률로 평가하는 모델이다.



언어 모델의 수식 표현
$$
P(w_1,...,w_m) = P(w_m|w_1,...,w_{m-1})P(w_{w-1},...,w_{m-2})...P(w_3|w_1,w_2,)P(w_2|w_1)P(w_1) = \prod^m_{t=1}P(w_t|w_1,...,w_{t-1})
$$




RNN

Encoder to Decoder (**Seq2Seq 모델)**

문맥 벡터를 통해 번역을 한다.

한 시계열 데이터를 다른 시계열 데이터로 변환(RNN 두 개를 연결(Encoder와 Decoder))

Encoder은 고정된 길이의 벡터로 문장을 인코딩함-마지막으로 나온 컨텍스트 벡터

Decoder 는 context 벡터를 이용해서 번역결과 출력

문제: 문장이 길어진다면? 컨텍스트 벡터가 모자라게 된다면..? 모든 정보를 담을 수 없음

해결: attention

---





![image-20210415095819061](C:\Users\Boyoon Jang\AppData\Roaming\Typora\typora-user-images\image-20210415095819061.png)

![](C:\Users\Boyoon Jang\AppData\Roaming\Typora\typora-user-images\image-20210415093854101.png)

---



# RNN(Recurrent Neural Network) 계열 모델

## RNN

순환하는 신경망. 순환하는 경로가 존재해서 데이터가 같은 장소를 반복해서 왕래할 수 있고, 데이터가 순환하면서 정보가 끊임없이 갱신되게 된다. RNN에 이용되는 계층을 RNN 계층이라고 부른다.





시계열 데이터 x0, x1...xt가 RNN 계층에 입력되고 입력에 대응하여 h0, h1...ht가 출력된다. 각 시각에 입력되는 xt는 벡터라고 가정한다. 자연어를 다루는 경우 각 단어의 분산 표현(단어 벡터)이 xt가 되며, 이 분산 표현이 순서대로 하나씩 RNN 계층에 입력된다. 

출력이 두 개로 분기하는 것은 같은 것이 복제되어 분기함을 의미. 이렇게 분기된 출력 중 하나가 자기 자신에 입력되어 순환함.





Recursive Neural Network(재귀 신경망)과 다른 것 유의





## RNN 모델의 장점

시퀀스 데이터를 순차적으로 넣으면 그 앞의 정보를 기억해서 예측하는 데에 도움을 준다.

RNN으로도 vector-sequence 모델, sequence-vector model, sequence-sequence 모델을 만드는 것 가능했다. 

**vector-sequence** 모델 예시: 입력이 그림이고 출력이 캡션인 모델. 그림을 벡터화해서 입력하면 시퀀스인 문장이 출력된다.

**sequence-vector** 모델 예시: 입력이 문장이고 출력이 확률 벡터인 감정 분석 모델. 문장을 입력하면 긍정적인지 부정적인지가 확률로서 출력된다.

**sequence-sequence** 모델: Encoder RNN과 Decoder RNN 모델 두 개를 연결해 놓은 것. 예시: 입력과 출력이 모두 시퀀스인 번역 모델

시계열 데이터 분석(특히 주가분석)은 아직도 LSTM이 효과적이라고 한다.

## RNN 계열의 모델의 단점

1) 훈련이 느리다. 입력 데이터가 순서대로 들어가야 하기 때문에 GPU를 잘 못 쓴다

2) 장기의존성 문제: 문장이 길어지면 과거의 단어를 반영하기 힘들어진다.

LSTM에서 장기의존성 문제를 일부 해결했지만, 완전히 해결하지 못했고, LSTM은 RNN보다도 훈련속도가 느렸다.




$$
h^{(t)}=f(h^{(t-1)},x^{(t)}; \theta)
$$
represents information seen before it.

근데 h^(t+1) 벡터는 d 차원의 정해진 크기의 벡터이고, 입력은 실수 공간의 어느 값이나 될 수 있어서 ht+1은 lossy vector이다.(그 전 상태의 모든 정보를 기억 못 함)

여기서 f(or U)는 transformation+activation의 기능을 함. 



현재 정보와 과거 정보를 담고 있는 히든 유닛 벡터
$$
a^{(t)} = Wh^{(t-1)}+Ux^{(t)}+b_u
$$
과거의 정보를 얼마나 담고 있는지는 이전 타임 스탭의 W 값에 달려있다.

복잡한 데이터를 모델링 하기 위해 비선형적인 활성화함수 사용
$$
h^{(t)}=tanh(a^{(t)})
$$

$$
h^{(t)}=tanh(Wh^{(t-1)}+Ux^{(t)}+b_u)
$$


$$
h^{(t)} h^{(t+1)}
$$

$$
x^{(t)} x^{(t+1)}
$$

$$
c^{(t)}= Vh^{(t)}+b_v
$$

$$
o^{(t)} = softmax(c^{(t)})
$$

$$
L^{(t)}= Loss(o^{(t)}, y^{(t)})
$$

$$
L = \sum_t L^{(t)}
$$



h들은 다른 시스템이 아니라 다른 시기의 같은 시스템

BPTT()

Teacher Forcing

진짜 레이블 y를 h에 넣음
$$
w^nx^{(0)} \rightarrow \begin{cases}
{\infin; \; W > 1}\\ {0; \; W<1} \end{cases}
$$

$$
\frac{\delta W^nx^{(0)}}{\delta W} \rightarrow \begin{cases} \infin; \; W > 1 \\0; \; W < 1 \end{cases}
$$

이를 일반화
$$
x^{(i)} \in \R^D \quad W \in \R^{D \times D}
$$
corresponding eigen vector to the n will explode -> input 값 잃게 됨
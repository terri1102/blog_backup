---
layout: default
title: 자연어 처리 1부                                                                 
date: 2021-04-05
categories:
  - Deep_Learning
tags:
  - Deep_Learing
comments: true
nav_order: 4
parent: Deep Learning
---



# NLP(Natural Language Processing)

### 자연어 처리란 무엇인가? 

프로그래밍 언어보다 비구조적인 일상 언어를 컴퓨터가 이해할 수 있는 형태로 변형하여 여러 분석을 수행하는 것이다. 자연어 처리 모델을 이용해서 할 수 있는 것은 문장 구문 분석, 감정 분석, 기계 번역, 질의응답 시스템, 문장 자동요약, 문장 생성 등 다양한다. 그런데 공부하면서 느낀 것이지만, 문장 구문 분석을 통해서 언어모델이 문장의 구문을 인식한 다음에 문장을 생성하거나 하는 건 아닌 것 같다. 엄청 휴리스틱하게, 그냥 이 단어 다음에는 이 단어가 올 확률이 높으니까 단어를 이어서 붙이고,  이 단어 후에는 문장이 끝날 확률이 높으니까 여기서 끊고..이런식으로 작동하는 것 같다. 엄청 방대한 학습 데이터를 쓰니까 그럴듯해보이는 결과를 낸다.  심지어 유명 모델들은 생성한 문장의 문법 같은 건 거의 완벽하게 나오는데, 상식이 부족한 대답을 하는 경우만 문제인 경우가 많다. 그래서 무조건 큰 학습 데이터를 구축하려는 것 같다. 



# NLP Process



<span class="fs-8">
[Unprocessed document]{: .btn }
</span> -> <span class="fs-8">
[1. Tokenization]{: .btn }
</span>-> <span class="fs-8">
[2.Remove Stop Words]{: .btn }
</span> -> <span class="fs-8">
[3. Stemming/Lemmatization]{: .btn }
</span>-><span class="fs-8">
[4.Normalization]{: .btn }
</span>-><span class="fs-8">
[Feature Extractor]{: .btn }
</span>-><span class="fs-8">[언어 모델 적용]{: .btn }</span>



NLP를 구조적으로 파악하기 위해 3부로 나눠서 정리해볼 예정인데, 이번 글에서는 1부 전처리 과정(1.Tokenization, 2. Remove Stop Words, 3. Stemming, 4. Normalization)에 대해 설명할 것이다. 



#### 자연어처리 관련 용어

- **코퍼스**(Corpus, 말뭉치):  특정한 목적을 가지고 수집한 텍스트 데이터
- **문서**(Document): 문장(Sentence)들의 집합
- **문장**(Sentence): 여러개의 **토큰**(단어, 형태소 등)으로 구성된 문자열 
- **어휘집합**(Vocabulary): 코퍼스에 있는 모든 문서, 문장을 토큰화한 후 중복을 제거한 토큰의 집합



# Preprocessing 전처리



## 1. Tokenization 텍스트 토큰화

**토큰:** 가장 낮은 단위로 어휘 항목을 구분할 수 있는 분류 요소. 의미를 가지는 최소한의 문자열. 단어나 형태소



#### 올바른 토큰화 체크리스트

- [ ]  반복 가능한 데이터 구조에 저장 되어야 함(의미단위(semantic unit) 분석을 위해)

- [ ]  가능하면 문자를 모두 대문자 or 소문자로 통일

- [ ]  문장 부호, 공백 등 영문자, 숫자가 아닌 문자 제거

```python
re.sub(r'[^a-zA-Z ^0-9]', '')
```

직접 전처리 방식을 코드로 구현해도 되고 여러 라이브러리의 tokenizer를 가져다가 써도 된다.

### Python으로 전처리하기

#### 토큰화하기

```python
#정제되지 않은 document가 입력됨
words = 'Random example for the demonstration.;'

#토큰화 함수 만들기(함수로 만들어 놓으면 나중에 사용하기 훨씬 편해짐)

def tokenize(doc):
	#특수 문자 제거
    import re
    regex = r'[^a-zA-Z0-9 ]'

    #치환할 문자
    subst = ""

    result = re.sub(regex, subst, words)

    #대소문자 정제
    tokens = result.lower()

    #iterate 가능한 구조로 저장
    tokens = tokens.split(" ") #리스트
    #re.split('(\W+)?'.text)로 자르면 단어 단위로 분할 가능


```



## 2. 불용어(stopword) 제거

단어 사전에 불용어가 없다면 토큰별 빈도를 이용해서 빈도가 너무 높은 토큰은 제거하는 방식으로 불용어를 제거할 수 있다.



#### 다양한 그래프로 시각화

```python
#token들의 누적 분포를 이용해서 그래프 
import seaborn as sns
sns.lineplot(x='rank', y = 'cul_percent', data= df)

#트리맵으로 토큰들 시각화
squarify.plot(sizes=wc_top20['percent'], label=wc_top20['word'], alpha=0.6)
plt.axis('off')
plt.show()
```



## 3. Stemming/Lemmatization

### spacy를 이용해서 전처리하기

```python
from spacy.lang.en import English

nlp = English()
 
doc = nlp('Hello world!')  #doc은 token들로 이루어짐

for token in doc:
    print(token.text)
    
token = doc[1]

print(token.text)
```

lexical attributes: 어휘적 특성

```python
#predicting part-of-speech tags
import spacy

#load the small English model
nlp = spacy.load("en_core_web_sm")

#process a text
doc = nlp("She ate the pizza")

#Iterate over the tokens
for token in doc:
    #print the text and the predicted part-of-speech tag
    print(token.text, token.pos_, token.dep_, token.head_text)
```

```python
#entities

#process a text
doc = nlp("Apple is looking at buying U.K. startup for $1 billion.")

#Iterate over the predicted entities
for ent in doc.ents:
    print(ent.text, ent.label_)
    
#spacy.explain('tag') #tag 의미 검색 가능
```



## 4. Normalization

텍스트를 공식적인 형태로 정제하는 것. 맞춤법에 맞지 않거나 띄어쓰기가 잘못된 단어들을 표준화된 형태로 바꿔서 같은 단어들로 인식할 수 있게 해준다.

ex)2moro->tomorrow, w/->with, gooood->good
---
layout: default
title: "자연어 처리 2부: 단어의 분산표현"
date: 2021-04-05
grand_parent: Deep Learning
parent: NLP
comments: true
nav_order: 2

---

# Distributional Representation 단어의 분산표현

지난 글에 이어 자연어 처리 2부 단어의 분산 표현(단어의 의미를 정확하게 파악할 수 있는 벡터 표현) 에 대한 글입니다. 



![DataRepresentation]



---



사람이 직접 단어의 의미를 정의하는 시소러스(Thesaurus)를 이용한 방법은 자주 안 쓰는 것 같기에...설명하지 않고, 여기서는 통계기반 기법과 추론 기법을 설명할 것이다. 통계 기반 기법으로 빈도를 이용한 코사인 유사도 등이 쓰이고, 추론 기반 기법으로 word2vec이 있다.



#### 단어의 분산 표현(Distributional representation)

단어의 분산 표현은 단어을 고정 길이의 밀집벡터(dense vector)로 표현한다. 밀집벡터란 대부분의 원소가 0이 아닌 실수인 벡터로, 앞으로 나올 코드에서 todense() 메서드로 구현되었다. 이런 단어의 분산 표현을 어떻게 구축할지 여러 방법이 있다.  그 중 분포가설과 분포 가서에 기초해 단어를 벡터로 나타내는 방법인 동시발생 행렬을 설명할 것이다.



**분포 가설:** 단어의 의미는 주변 단어들에 의해 결정된다는 가설. 단어가 사용된 맥락을 고려하기 때문에 단어 간 물리적 거리를 이용하여 벡터화한다.

**동시발생 행렬:** 분포 가설을 기초로 하여 단어를 벡터화할 때 주변 단어를 세어서 통계화한 값을 행렬화(상관관계 매트릭스 같이 생겼음)



단어를 벡터로 표현할 때 단어의 의미를 얻는 방법은 서로 다르지만, '통계 기반 기법'과 '추론 기반 기법' 의 배경에는 모두 분포 가설이 있다.



<img src="https://github.com/terri1102/terri1102.github.io/blob/master/assets/images/%ED%86%B5%EA%B3%84%EC%99%80%EC%B6%94%EB%A1%A0.png?raw=true" alt="통계와추론" style="zoom:67%;" />

[^ ]: 밑바닥부터 시작하는 딥러닝2 p.115



## 텍스트 문서의 벡터화 방법

## 1.빈도를 이용한 방법

#### :CountVectorizer,  TF-IDF, SVD, LSA Embedding

Bang-of-Words(BoW): 단어의 순서를 무시하고 빈도만 고려하는 모델. 문서를 토큰화한 후 토큰의 빈도를 기반으로 벡터화한다. 데이터프레임 형태로 보자면 행은 각 문서가 되고 열은 중복되지 않는 각 단어가 됩니다. 열에는 단순히 각 단어가 문서에 얼마나 존재하는지를 카운트한 값을 넣거나(*CountVectorizer* 사용) TF-IDF 값이 오게 할 수 있습니다(*TfidfVectorizer 사용).



### CountVectorizer

```python
from sklearn.feature_extraction.text import CountVectorizer

# 문장들을 리스트로 입력 
text = ["This sentence is written only for CountVectorizer demo","sentence2..."]

# Instantiate
vectorizer = CountVectorizer()

# text를 기반으로 어휘 사전 생성
vectorizer.fit(text) 
#vectorizer.vocabulary_ : 모든 토큰과 맵핑된 인덱스 정보 나옴
#vectorizer.get_feature_names() #토큰들 리스트

# text를 DTM(document-term matrix)으로 변환(transform)
dtm = vectorizer.transform(text)
#compressed sparse row matrix(csr) 형태, 0을 표현하지 않음
#dtm.todense()를 쓰면 0을 표현하게 되고 numpy.matrix가 됨
```

| dtm                                                          | dtm.todense()                                                |
| ------------------------------------------------------------ | ------------------------------------------------------------ |
| <img src="https://github.com/terri1102/terri1102.github.io/blob/master/assets/images/n422_dtm.png?raw=true" alt="n422_dtm" style="zoom:70%;" /> | <img src="https://github.com/terri1102/terri1102.github.io/blob/master/assets/images/n422_todense.png?raw=true" alt="n422_todense" style="zoom:70%;" /> |



#### 데이터 프레임으로 보기

```python
from sklearn.feature_extraction.text import CountVectorizer

vectorizer = CountVectorizer(stop_words='english' #영어 불용어 처리
                       , max_features=10000) #빈도순으로 10000개 단어만 사용
# fit & transform
dtm = vectorizer.fit_transform(data)

dtm = pd.DataFrame(dtm.todense(), columns=vectorizer.get_feature_names())
dtm.shape
```



## TF-IDF

어떤 단어,문장이 주어졌을 때 각 단어별로 문장의 연관성을 수치로 나타낸 값

TF스코어 

 Term Frequency: 한 문장 내에서 그 단어가 얼마나 자주 출현하는지를 나타내는 척도
$$
tf(t,d) = \frac{Term\; t\; frequency\; in\; document}{Total\; words\; in\; document}
$$




IDF
$$
IDF = log {(총 \space 문장 \space 개수) \over {(단어가 \space 출현한 \space 문장의 \space 개수)}}
$$

$$
idf(t) = log(\frac{n}{1+df(t)})
$$



  \- 왜 IDF 스코어를 추가로 고려할까요? 

  Inverse Document Frequency 

한 단어가 문서 집합 전체에서 얼마나 공통적으로 나타나는지를 나타내는 값이다. 전체 문서의 수를 해당 단어를 포함한 문서의 수로 나눈 뒤 로그를 취하여 얻을 수 있다.

 모든 문장에서 나타나는 단어는 불용어 등 중요한 단어가 아닐 수 있기에. 

### 통계 기반 기법

통계 기반 기법은 대량의 텍스트 데이터인 말뭉치를 이용해서 통계적(단어 **빈도**)으로 문장 내에서 단어의 의미를 유추하는 방법이다. 즉, 주번 단어의 빈도를 기초로 단어를 표현하는데, 단어의 동시발생 행렬을 만들고 그 행렬에 SVD를 적용하여 밀집벡터(단어의 분산 표현)를 얻었다.

단어의 분산 표현: 단어의 의미를 정확히 파악할 수 있는 벡터 표현



통계 기반 기법의 문제점

거대한 학습 데이터에 SVD를 적용하면 연산량이 엄청나다. SVD를 n x n 행렬에 적용하는 비용은 O(n^3)이다. 



추론 기반 기법과 비교

통계 기반 기법은 말뭉치 전체의 통계(동시발생 행렬과 PPMI 등)를 이용해 단 1회의 처리(SVD 등)만에 단어의 분산을 얻는데 추론 기반 방법은 미니 배치로 한번에 소량의 학습 샘플씩 반복하여 가중치를 갱신해간다. 따라서 말뭉치의 어휘 수가 많아서 SVD 등 계산량이 큰 작업을 처리하기 어려운 경우에도, 신경망을 학습시킬 수 있다. 또한 여러 GPU를 이용해서 병렬 계산도 가능하기에 학습 속도를 높일 수 있다.



### 추론 기반 기법

추론 기반 기법은 추측하는 것이 목적이며, 그 부산물로 단어의 분산 표현을 얻을 수 있다. 

Word2Vec은 추론 기반 기법이며, 단순한 2층 신경망이다. 





## GloVe

추론 기반 기법과 통계 기반 기법을 융합한 기법.

Glove의 기본 아이디어는 말뭉치 전체의 통계 정보를 손실 함수에 도입해 미니 배치를 하는 것이다. 

---







---

맥락을 고려하는 워드 임베딩를 사용하는 word2vec 방법과 시퀀스 데이터 분석 방법으로 볼 수 있습니다. 워드 임베딩은 로 데이터를 내가 원하는 벡터 공간으로 매핑시켜주는 것인데요, (맥락을 고려하는 임베딩인) Word2vec 방법으로 CBOW와 Skip gram 방법을 배웠습니다.

둘의 개념을 간단히 설명하면 CBOW는 주변 단어를 통해 타겟 단어를 예측하는 것이고, Skip gram은 타겟 단어로  주변 단어를 예측하는 것입니다. (CBOW는 결괏값이 타겟 하나이기에 연산이 빠르고 작은 corpus size일 때 적합하며, skip gram은 조금 느리지만 큰 차원의 데이터를 사용할 때 적합합니다.)

(이렇게 벡터화된 데이터를 분석하는)시퀀스 데이터 분석 방법은 RNN, LSTM, GRU 방법을 배웠는데, RNN은 (시간축을 고려해서) 은닉층의 출력값이 입력값으로 들어가는 것이 반복되는 재귀적 반복이 일어나는 모델입니다. LSTM은 RNN의 가중치가 재귀적으로 곱해지면서 기울기(그래디언트) 소실/폭발 문제를 해결하기 위해 셀을 통해 과거 데이터도 이용해서 기울기를 조절하는 Gate를 달아준 것이고, GRU는 LSTM에서 gate 줄이고 이를 컨트롤러로 조절하는 모델이라고 할 수 있습니다. 



## Sentence Embedding





---

### 차원 축소와 관련해서 생각해 볼 거리

SVD나 다른 차원 축소와 완벽하게 대응되는 얘기는 아니지만, 동영상을 보고 인상깊어서 여기에 정리해둔다.

> 데이터에서 정보를 찾아내는 것은 패턴 찾기입니다. 
>
> 정보라는 건 뭘까요? 기본적으로는 순서에 관한 문제로 보여요.
>
> 글자의 순서를 통해 단어를 만들고, 단어의 순서를 통해 어떤 정보를 갖는 문장을 만들어내죠. 그러니 기본적으로 정보는 어떤 순서에 대한 것일까요? (information = order?)
>
> 한 단어의 모든 글자가 같은 양의 정보를 가지고 있을까요? 아니에요. q 다음에는 주로 u가 오는 것처럼, th 다음에는 주로 e가 오는 것처럼, q와 th 같은 글자들은 아주 작은 양의 정보만을 가지고 있어요. 왜냐하면 그 전에 예상할 수 있기 때문이에요. 불필요한 부분이죠.
>
> 정보론의 창시자, claude shannon은 영어의 불필요한 부분을 약 75%로 추정했어요. 그러니 영어가 압축적일 수 있는 거죠. 임의적이지 않으니까요. 패턴을 가지고 있어요. 규칙이 있다는 건 압축 가능(data moshing)하다는 겁니다. 



> 그렇지만 우리가 가장 많이 압축할 수 있는 건 뭘까요? 뭐든 임의적이지 않은 것들입니다. 어떤 패턴도 규칙까지도 그것들이 예측할 수 있으면 줄일 수 있습니다. 
>
> 만약 어떤 파일을 끊임없이 간추려낸다면 마지막에 남는 것은 완전한 임의가 됩니다. 즉, 만약 어떤 파일을 끊임없이 간추려낸다면(compress) 마지막에 남는 것은 완전한 랜덤이 되는 것입니다. 그리고 그건 본래 있던 것의 모든 정보를 담고 있을 것입니다. 정제된 순수한 형태로 말이죠. 
>
> 데이터가 얼마나 많은 정보를 포함하고 있는지 알려면 그 정보들이 얼마나 랜덤한지 알아보면 됩니다.  (Pure information is random)
>
> Randomness = Disorder = Entropy 이기 때문에 Information = Entropy입니다.
>
> 각각의 정보가 독립적이고 전부 임의적이라면 최대의 정보(엔트로피)를 가지고, 더이상 예측 불가해집니다.

[^ ]: veritasium <임의적이지 않다는 게 뭘까요?>



선형종속적인 벡터들을 다 축소하고 나면, 남는 데이터들은 선형독립적일 텐데, 그럼 그 정보들끼리는 서로 관련이 적고, 랜덤하다고 볼 수 있을듯. 그럼 이렇게 정제된 정보들을 분석하는 건 인간 or 신경망으로 관련없어 보이는 것의 패턴을 다시 찾거나, 아니면 다른 정보로 보고 우선순위를 정하거나 할 수 있을 것 같다. '중요한 정보'만 남기면서 차원 축소를 할 때 남게 되는 정보들의 관계에 대해 생각해 볼 수 있어서 좋았다.
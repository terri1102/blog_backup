---
layout: post
title: "[Machine Learning] Gradient Boosting"
date: 2021-02-17
category: [Machine Learning]
MachineLearning : true 
parent: Machine Learning
tags: [AdaBoost, XGBoost, LightGBMBoost]
comments: true
---



# Boosting

부스팅은 배깅(Bagging)과 다르게 이미 만들어진 트리의 결과가 다음 트리의 예측에 영향을 주는 것을 의미한다. 



# AdaBoost

--잘 모르겠지만 많이 안 쓰는 것 같음...



**특징** 

1.  AdaBoost는 weak learners들을 결합하며, 대부분의 경우 weak learners 들은 하나의 노드, 두 개의 leaf  를 가지는 stump다.

2. 일부 stumps는 분류를 할 때 더 큰 영향력을 가진다.

3. 각 stump는 이전 stump의 실수를 고려한다.



**각 stump의 영향력 계산(각 stump의 가중치)**
$$
Amount\; of \;say = \frac{1}{2}log(\frac{1-Total Error}{Total Error})
$$



**다음 stump가 받는 sample weight**

1) 잘못 예측했을 때 가중


$$
New \;Sample\;Weight = sample \;weight * e^{amount \;of\;say}
$$



2) 제대로 예측했을 때 경감


$$
New \;Sample\;Weight = sample \;weight * e^{-amount \;of\;say}
$$



weighted gini index는 이런 샘플 간 중요도 차이를 고려하게 된다. balanced로 해도 되고, 직접 정해 줄 수도 있다. 

New Sample Weight를 고려해서 원래 샘플 사이즈와 동일한 사이즈의 샘플 세트를 만들고, 각 stump의 amount of say를 다 더해서 더 큰 쪽의 의견을 따른다.



------



# Gradient Boost

에다부스트와 차이: 샘플 간 가중치를 주지 않고, 첫 트리는 X와 y로 훈련 시킨 후 이 트리의 오차를 타겟으로 바꾼꿔서 다시 트리 모델을 만든다. 각 샘플의 오차를 줄이기 위해서 타겟으로 계속 사용함. 다음 트리는 오차가 큰 샘플에 가중치를 두게 된다. '오차를 예측한다'?



[step1] 처음에 leaf를 만든다. 보통 평균을 쓴다. 

[step 2] 이를 이용해서 트리를 만든다. 트리의 사이즈는 adaboost 보다 보통 크지만 트리의 크기(leaf의 개수)는 제한되어 있다.  여기서 구한 값에 learning rate를 곱한 후 첫 leaf와 더한다.

[step3] 기존 트리의 실수를 고려해서 새 트리를 만든다. 여기서 구한 값에 learning rate를 곱한 후 첫 leaf, 기존 트리에서 구한 값과 더한다.



Pseudo Residual

관측치 - 평균



Learning rate를 설정한다. 모든 트리를 동일한 비율로 scale한다.





회귀는 MSE 많이 씀



분류에도 사용 가능

log loss 사용?



# XGBoost



XGBoost: CART 기반의 부스팅 앙상블 알고리즘



gradient decent를 쓰기 때문에 gradient boost



다중분류랑 이진분류는 비용함수가 다르다. 나중에 섹션4에서 배움

scale pos weight 타겟 비율 차이

class weight 모든 클래스 비중 차이



샵과 모델의 예측력이 다르며, 샵의 예측이 더 좋다? 둘다 다른 값 나옴



타겟일 때 로그변환?

데이터의 분포를 확률적으로 구하는 거니까



특성도 치우쳐있으면 로그변환?

트리를 학습시킬 때 로그변환한 특성을 쓰면 별 차이가 없지 않나?  



차이가 없는 데이터를 계속 학습시키면 

편차의 차이를 학습해서 값을 내는 거니까 보통은 더 잘 되는 거라고 함



r2 score가 높으면 특성을 하나씩 넣어보기



multi class classifier

weighted f1? macro, micro f1? 언제 써야하나?  통계학 문제 

micro는 accuracy랑 같음 

 마이크로 평균은 각 샘플이나 예측에 동일한 가중치를 부여하고자 할 때 사용한다.  매크로 평균은 모든 클래스에 동일한 가중치를 부여하여 분류기의 전반적인 성능을 평가한다. 이 방식에서는 가장 빈도 높은 클래스 레이블의 성능이 중요하다. 사이킷런에서 이진 성능 지표로 다중 분류 모델 평가하면 정규화 또는 가중치가 적용된 매크로 평균이 기본으로 적용된다. 가중치가 적용된 매크로 평균은 평균을 계산할 때 각 클래스 레이블의 샘플 개수를 가중하여 계산한다. 가중치 적용된 매크로 평균은 레이블마다 샘플 개수가 다른 불균형한 클래스 다룰 때 유용하다.

---



# Lightgbm

```python
import lightgbm as lgb

#X_train, X_test, y_train, y_test 다 준비된 상태

params = {
    "max_bin": 512,
    "learning_rate": 0.05,
    "boosting_type": "gbdt",
    "objective": "binary",
    "metric": "binary_logloss",
    "num_leaves": 10,
    "verbose": -1,
    "min_data": 100,
    "boost_from_average": True
}

model = lgb.train(params, d_train, 10000, valid_sets=[d_test], early_stopping_rounds=50, verbose_eval=1000)
```



catboost 

pool 전처리 용



---



## References

AdaBoost, Clearly Explained https://youtu.be/LsK-xG1cLYA
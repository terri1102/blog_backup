---
layout: default
title: "PCA와 공분산행렬"
date: 2021-01-14
parent: Machine Learning
nav_order: 2
tags: [Eigen vector, Eigen values, PCA, plot]
comments: true
---



# PCA(Principal Component Analysis)

---

**PCA**(주성분분석)란 데이터를 정사영시켜 차원을 축소하면서도 기존의 데이터 구조를 유지하는 기법이다. 실제로 수집되는 데이터들은 많은 특성을 가지고 있기 때문에 데이터의 차원 축소가 필요하다.

또한, **차원의 저주**로 인해 데이터의 차원이 증가할 수록 모델이 복잡해지는 overfitting 문제가 발생하게 되기 때문에 데이터 양에 비해 차원이 방대한 것을 피하고자 한다. overfitting은 feature의 개수가 sample의 개수보다 많을 때 발생한다. PCA는 Feature의 개수를 줄이는 Feature extraction의 한 방법이다. (<-> Feature selection )



## PC를 찾는 과정

---

1. **분산 보존**: 분산이 최대가 되는 축 찾기

PCA는 데이터의 분산이 최대가 되는 축을 찾는다. 제일 분산이 큰 벡터를 기준으로 transformation해야 잃는 정보가 적어지기 때문이다. 간단히 말해 분산 값이 클수록 feature가 갖고 있는 정보가 많다고 할 수 있다.  데이터의 분산을 가장 크게 만드는 축은 원본 데이터와 투영된 데이터 간의 평균제곱거리를 최소화하는 축이다. 즉, PC i(i번째 주성분)= i번째 축을 정의하는 단위 벡터이다.

2. 첫번째 축과 직교하면서 분산이 최대인 두번째 축 찾기
3. 첫번째 축과 두번째 축과 직교하면서 분산이 최대인 세 번째 축 찾기...계속 반복



## PCA 과정

---

1. 데이터를 **정규화** 하기: 각 행에서 feature의 평균을 빼고, 이를 표준편차로 나누어준다.
2. **공분산** 구하기(Covariance): 정규화된 데이터(Z)의 분산-공분산 매트릭스를 계산한다.
3. 분산-공분산 매트릭스의 고유벡터를 계산한다. 그 중 분산값이 큰 것부터 순서대로 나열한다.

sklearn의 PCA 는 고유값 분해(eigenvalue-decomposition)가 아닌  특이값 분해(SVD, Singular value decomposition)를 이용한다. 하지만 sklearn은 아이겐 값과 아이겐벡터도 제공한다.

PCA가 어떤 축이 가장 좋은 fit인지 아는 방법: 

<img src="C:\Users\Boyoon Jang\Desktop\Repository\terri1102.github.io\assets\img\pca.PNG" style="zoom:67%;" />

[^ ]: statQuest

vector projection을 통해서 거리를 구한다. 이는 피타고라스 정리에 따라 원점과 project된 선의 거리(c)가 멀어질수록 project된 선과 자료들 간의 거리(b)가 줄어들기 때문에, c를 최대화하는 선을 찾는다.(a는 고정) 이때 원점과 project된 선의 거리(c)를 구하는 게 project된 선과 자료들 간 거리(b)를 구하는 것보다 쉬움

4. 데이터를 고유벡터에 투영한다.

<img src="C:\Users\Boyoon Jang\Desktop\Repository\terri1102.github.io\assets\img\pca2.PNG" alt="pca2" style="zoom:60%;" />

 *sum the squared point distances(SS): 위의 그림에서 무수히 많은 데이터의 c값의 제곱의 합

PC1는 SS가 가장 큰 선이다. PC2는 PC1과 원점에서 수직으로 만나면서 SS가 두번째로 큰 선이다.

Eigenvalue=SS

SS(distances for PC2)=Eigenvalue for PC2

Variation for PC1 = SS(distance for PC1)/n-1

Variation for PC2 = SS(distance for PC2)/n-1

PCA의 variation은 위 둘을 더한 것



**Scaling**

standardization: 평균 0 표준편차1로 만듦, sklearn의 PCA는 이 방식.

minmax scaling: 값을 0~1사이로 반환, 아웃라이어 있을 떄 날뛰는 값의 영향 더 줄일 수 있음

normalization: 값들을 특정 범위, 주로 [0,1] 사이로 스케일링

[^ ]: https://m.blog.naver.com/mrp/221672080759



---

**PCA 결과해석**

feature 가 너무 많다면 어떻게 해야할까?

Cluster: 서로 높은 상관성을 갖는 세포들을 하나의 군집으로 묶는다.

PC1은 x축, PC2는 y축으로 할 때, PC1가 더 중요하기 때문에 PC1에서 클러스터끼리의 거리가 PC2 기준 클러스터 끼리의 거리와 동일하다고 하더라도 PC1에서의 차이가 더 크다고 볼 수 있다.



몇 차원으로 축소해야 최적인가? K means 로 고민



---



# sklearn으로 PCA 수행 

1. 데이터 프레임을 array로 바꾼다.

```python
new_df = pd.DataFrame.to_numpy(df)
```

2. 정규화 시키기

```python
from numpy import array          #얘는 어디쓰이는지 잘 모르겠네
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA

scaler = StandardScaler()
Z = scaler.fit_transform(new_dr)
#print("\n Standardized Data:\n", Z)
```

3. PCA

```python
pca = PCA(2)                #PC의 개수 설정, 나중에 principalDf의 column 개수랑 맞춰야 함
pca.fit(Z)
```

4. PCA components

```python
principalComponents = pca.fit_transform(Z)
principalDf = pd.DataFrame(data=principalComponents, columns = ['PC1, PC2'])
#print("\n Eigenvectors: \n", pca.components_)
print("\n Eigenvalues: \n", pca.explained_variance_)
print("\n Explained variance ratio: \n", sum(pca.explained_variance_ratio_))
```

---



**PCA 참고용**

fit(X[, y]): Fit the model with X.

fit_transform(X[, y]): Fit the model with X and apply the dimensionality reduction on X.

get_covariance(): Compute data covariance with the generative model.

get_params([deep]): Get parameters for this estimator.

get_precision(): Compute data precision matrix with the generative model.

inverse_transform(X): Transform data back to its original space.

score(X[, y]): Return the average log-likelihood of all samples.

score_samples(X): Return the log-likelihood of each sample.

set_params(**params): Set the parameters of this estimator.

transform(X): Apply dimensionality reduction to X.d





### scikit learn에서 사용하는 fit, transform의 의미

---

Scikit learn에서는 대부분의 로직에서 fit()과 transform()을 쌍으로 사용한다. 

ex) sklearn.preprocessing의 StandardScaler, MinMaxScaler, PCA클래스, 텍스트의 Feature Vectorization 클래스들(CountVectorizer, TFIDF등) 

학습데이터 세트에서 변환을 위한 기반 설정을 먼저 fit()을 통해서 설정한 뒤에 이를 기반으로 학습 데이터의 transform()을 수행하되 학습 데이터에서 설정된 변환을 위한 기반 설정을 그대로 테스트 데이터에도 적용하기 위해서 그렇다.

즉 학습 데이터 세트로 fit() 된 Scaler를 이용하여 테스트 데이터를 변환할 경우에는 테스트 데이터에서 다시 fit()하지 않고 반드시 그대로 이 Scaler를 이용하여 transform()을 수행해야 한다.

train data를 scaling하고 fit, transform 한 후 classifier로 학습했다면 예측 시 먼저 테스트 데이터 세트를 반드시 학습 데이터로 Scaling된 MinMaxScaler를 이용하여 fit()한 뒤 transform() 해야된다. 즉 학습할 때와 동일한 기반 설정으로 동일하게 테스트 데이터를 변환해야 하는 것이다. 학습 데이터에서 Scale된 데이터를 기반으로 Classifier가 학습이 되었기 때문에 이렇게 학습된 Classifier가 예측을 할 때에도 학습 데이터의 Scale 기준으로 테스트 데이터를 변환 한 뒤 predict해야한다.


\# 학습 데이터에 대해서 fit(), transform() 수행.
scaler.fit(X_train)
scaled_X_train = scaler.transform(X_train)

\# 테스트 데이터에서는 다시 fit(), transform()이나 fit_transform()을 수행하지 않고 transform만 수행.
scaled_X_test = scaler.transform(X_test)

fit_transform()을 fit() 과 transform() 함께 수행하는 메소드



하지만 테스트 데이터에 scaled_X_test = scaler.fit_transform(X_test)를 적용해서는 안 된다. 이를 수행하면 scaler 객체가 기존에 학습 데이터에 fit 했던 기준을 모두 무시하고 다시 테스트 데이터를 기반으로 기준을 적용하기 때문이다. 

때문에 테스트 데이터에 fit_transform()을 적용해서는 안 된다.

이런 번거로움을 피하기 위해 학습과 테스트 데이터로 나누기 전에 먼저 Scaling등의 데이터 전처리를 해주는 것이 좋다.  학습과 테스트 데이터로 나누기 전에 Scaling을 적용하면 fit(), transform()을 순차적으로 써도 좋고 fit_transform()으로 변환해도 무방하다.

[^ ]: 출처: 인프런 질답 https://www.inflearn.com/questions/19038